#!/usr/bin/env python3

import argparse
import csv
import glob
import json
import os
import re
import sys
import yaml

from pathlib import Path

def jdump(x):
    print(json.dumps(x, indent=2))


def join_indexed_dicts(dict_1, dict_2):
    """
    dict_1 is a dict of dicts, where the keys of the outer dict are the index
    dict_2 is a dict of dicts, where the keys of the outer dict are the index
    Returns a dict of dicts, where the keys of the outer dict are the index, and the values are the union of the inner dicts

    :param dict_1: dict of dicts
    :type dict_1: dict
    :param dict_2: dict of dicts
    :type dict_2: dict
    :return: dict of dicts
    :rtype: dict
    """
    joined_dict = {}
    if not dict_1:
        return dict_2
    if not dict_2:
        return dict_1
    dict_1_index = set(dict_1.keys())
    dict_2_keys = list(dict_2.values())[0].keys()
    
    for k in dict_1_index:
        joined_dict[k] = dict_1[k]
    for k in dict_1_index:
        if k in dict_2.keys():
            for k2 in dict_2_keys:
                joined_dict[k][k2] = dict_2[k][k2]
        else:
            for k2 in dict_2_keys:
                joined_dict[k][k2] = None
            
    return joined_dict


def parse_provenance(provenance_path: Path) -> list[dict]:
    """
    Parse a provenance file and return a list of dicts

    :param provenance_path: path to provenance file
    :type provenance_path: Path
    :return: list of dicts
    :rtype list[dict]
    """
    with open(provenance_path, 'r') as f:
        try:
            parsed_provenance = yaml.safe_load(f)
        except yaml.YAMLError as e:
            print(e)
            exit(-1)
    for provenance_item in parsed_provenance:
        if 'timestamp_analysis_start' in provenance_item.keys():
            provenance_item['timestamp_analysis_start'] = str(provenance_item['timestamp_analysis_start'])

    return(parsed_provenance)


def parse_assembly_provenance(provenance_path: Path) -> dict:
    """
    Parse an assembly provenance file and return a dict of assembly provenance info

    :param provenance_path: path to assembly provenance file
    :type provenance_path: Path
    :return: dict of assembly provenance info
    :rtype: dict
    """
    parsed_provenance = parse_provenance(provenance_path)
    assembly_provenance = {}
    for provenance_item in parsed_provenance:
        if 'pipeline_name' in provenance_item:
            assembly_provenance['assembly_pipeline_name'] = provenance_item['pipeline_name']
        if 'pipeline_version' in provenance_item:
            assembly_provenance['assembly_pipeline_version'] = provenance_item['pipeline_version']
        if 'process_name' in provenance_item and provenance_item['process_name'] == 'unicycler':
            tools = provenance_item['tools']
            for tool in tools:
                if tool['tool_name'] == 'unicycler':
                    assembly_provenance['assembly_tool_name'] = tool['tool_name']
                    assembly_provenance['assembly_tool_version'] = tool['tool_version']
        if 'process_name' in provenance_item and provenance_item['process_name'] == 'plassembler':
            tools = provenance_item['tools']
            for tool in tools:
                if tool['tool_name'] == 'plassembler':
                    assembly_provenance['assembly_tool_name'] = tool['tool_name']
                    assembly_provenance['assembly_tool_version'] = tool['tool_version']

    return assembly_provenance


def parse_mlst_provenance(provenance_path: Path) -> dict:
    """
    Parse an mlst provenance file and return a dict of mlst provenance info

    :param provenance_path: path to mlst provenance file
    :type provenance_path: Path
    :return: dict of mlst provenance info
    :rtype: dict
    """
    parsed_provenance = parse_provenance(provenance_path)
    mlst_provenance = {}
    for provenance_item in parsed_provenance:
        if 'pipeline_name' in provenance_item:
            mlst_provenance['mlst_pipeline_name'] = provenance_item['pipeline_name']
        if 'pipeline_version' in provenance_item:
            mlst_provenance['mlst_pipeline_version'] = provenance_item['pipeline_version']

    return mlst_provenance


def parse_taxon_abundance_provenance(provenance_path: Path) -> dict:
    """
    Parse a taxon abundance provenance file and return a dict of taxon abundance provenance info

    :param provenance_path: path to taxon abundance provenance file
    :type provenance_path: Path
    :return: dict of taxon abundance provenance info
    :rtype: dict
    """
    parsed_provenance = parse_provenance(provenance_path)
    taxon_abundance_provenance = {}
    for provenance_item in parsed_provenance:
        if 'pipeline_name' in provenance_item:
            taxon_abundance_provenance['species_pipeline_name'] = provenance_item['pipeline_name']
        if 'pipeline_version' in provenance_item:
            taxon_abundance_provenance['species_pipeline_version'] = provenance_item['pipeline_version']

    return taxon_abundance_provenance


def parse_sample_qc_summary(sample_qc_summary_path: Path) -> dict:
    """
    Parse a sample qc summary csv file and return a dict of sample qc summary info
    
    Note: the "sample_qc_summary.csv" file was introduced in auto-cpo v0.1.0, and
    has a different structure from the "qc_summary.csv" file. This function is
    intended to parse the new "sample_qc_summary.csv" file.

    :param sample_qc_summary_path: path to sample qc summary csv file
    :type sample_qc_summary_path: Path
    :return: dict of sample qc summary info
    """
    sample_qc_summary = []
    int_fields = [
        'total_bases_input',
        'filtered_bases_input',
    ]
    float_fields = [
        'pre_alignment_estimated_depth_coverage_per_mb',
        'q30_percent_before_filtering',
        'q30_percent_after_filtering',
    ]

    with open(sample_qc_summary_path, 'r') as f:
        reader = csv.DictReader(f, dialect='unix')
        for row in reader:
            sample_qc_summary_record = {}
            for k, v in row.items():
                if k in int_fields:
                    try:
                        sample_qc_summary_record[k] = int(v)
                    except ValueError:
                        sample_qc_summary_record[k] = None
                elif k in float_fields:
                    try:
                        sample_qc_summary_record[k] = float(v)
                    except ValueError:
                        sample_qc_summary_record[k] = None
                else:
                    sample_qc_summary_record[k] = v
            sample_qc_summary.append(sample_qc_summary_record)

    return sample_qc_summary


def parse_qc_summary(qc_summary_path: Path) -> dict:
    """
    Parse a qc summary csv file and return a dict of qc summary info

    :param qc_summary_path: path to qc summary csv file
    :type qc_summary_path: Path
    :return: dict of qc summary info
    :rtype: dict
    """
    qc_summary = []
    int_fields = [
        'total_bases_input',
    ]
    with open(qc_summary_path, 'r') as f:
        reader = csv.DictReader(f, dialect='unix')
        for row in reader:
            qc_summary_record = {}
            for k, v in row.items():
                if k in int_fields:
                    try:
                        qc_summary_record[k] = int(v)
                    except ValueError:
                        qc_summary_record[k] = None
                else:
                    qc_summary_record[k] = v
            qc_summary.append(qc_summary_record)

    return qc_summary
    

def parse_nanopore_basic_qc_stats(nanopore_basic_qc_stats_path: Path) -> dict:
    """
    Parse a nanopore basic qc stats csv file and return a dict of basic qc stats.

    :param nanopore_basic_qc_stats_path: path to nanopore basic qc stats csv file
    :type nanopore_basic_qc_stats_path: Path
    :return: dict of basic qc stats
    :rtype: dict
    """
    basic_qc_stats = []

    int_fields = [
        'reads',
        'bases',
        'n50',
        'mean_length',
        'median_length',
    ]

    float_fields = [
        'mean_quality',
        'median_quality',
    ]
        
    with open(nanopore_basic_qc_stats_path, 'r') as f:
        reader = csv.DictReader(f, dialect='unix')
        for row in reader:
            basic_qc_stats_record = {}
            for k, v in row.items():
                if k in int_fields:
                    try:
                        basic_qc_stats_record[k] = int(v)
                    except ValueError:
                        basic_qc_stats_record[k] = None
                elif k in float_fields:
                    try:
                        basic_qc_stats_record[k] = float(v)
                    except ValueError:
                        basic_qc_stats_record[k] = None
                else:
                    basic_qc_stats_record[k] = v
            basic_qc_stats.append(basic_qc_stats_record)

    return basic_qc_stats


def harmonize_input_reads_qc(input_short_reads_qc: list[dict], input_long_reads_qc: list[dict]) -> dict:
    """
    Harmonize input reads qc summary info, so that it is in a consistent format
    with a standard set of keys, regardless of the type of sequencing run, assembly mode or
    which qc version of qc summary file was generated.

    Take both short and long reads qc summary info, and return a dict of harmonized input
    reads qc summary info, for each library id.

    :param input_reads_qc: list of input reads qc summary info
    :type input_reads_qc: list[dict]
    :return: dict of harmonized input reads qc summary info, indexed by library id
    :rtype: dict
    """
    output_fields = [
        'library_id',
        'total_bases_input_short',
        'total_bases_input_long',
    ]
    harmonized_input_reads_qc_by_library_id = {}
    for input_reads_qc_record in input_short_reads_qc:
        if 'library_id' in input_reads_qc_record.keys():
            library_id = input_reads_qc_record['library_id']
        elif 'sample_id' in input_reads_qc_record.keys():
            library_id = input_reads_qc_record['sample_id']
        else:
            continue
        if library_id not in harmonized_input_reads_qc_by_library_id.keys():
            harmonized_input_reads_qc_by_library_id[library_id] = {
                'library_id': library_id,
                'total_bases_input_short': 0,
                'total_bases_input_long': 0,
            }
        if 'total_bases_before_filtering' in input_reads_qc_record.keys():
            harmonized_input_reads_qc_by_library_id[library_id]['total_bases_input_short'] = input_reads_qc_record['total_bases_before_filtering']
        elif 'total_bases_input' in input_reads_qc_record.keys():
            harmonized_input_reads_qc_by_library_id[library_id]['total_bases_input_short'] = input_reads_qc_record['total_bases_input']

    for input_reads_qc_record in input_long_reads_qc:
        if 'library_id' in input_reads_qc_record.keys():
            library_id = input_reads_qc_record['library_id']
        elif 'sample_id' in input_reads_qc_record.keys():
            library_id = input_reads_qc_record['sample_id']
        else:
            continue
        if library_id not in harmonized_input_reads_qc_by_library_id.keys():
            harmonized_input_reads_qc_by_library_id[library_id] = {
                'library_id': library_id,
                'total_bases_input_short': 0,
                'total_bases_input_long': 0,
            }
        if 'bases' in input_reads_qc_record.keys():
            harmonized_input_reads_qc_by_library_id[library_id]['total_bases_input_long'] = input_reads_qc_record['bases']

    return harmonized_input_reads_qc_by_library_id



def find_and_parse_input_reads_qc(assembly_mode_output_dir: Path) -> dict:
    """
    Depending on which type of assembly was performed, first find the appropriate qc summary file, then
    dispatch to the appropriate parsing function and return the parsed qc summary info.

    :param assembly_mode_output_dir: path to assembly mode output directory
    :type assembly_mode_output_dir: Path
    :return: dict of qc summary info
    :rtype: dict
    """
    assembly_mode = os.path.basename(assembly_mode_output_dir)
    sequencing_run_output_dir = os.path.dirname(assembly_mode_output_dir)
    sequencing_run_id = os.path.basename(sequencing_run_output_dir)
    input_short_reads_qc = []
    input_long_reads_qc = []
    if assembly_mode == 'short':
        possible_qc_summary_filenames = [
            sequencing_run_id + '_auto-cpo_qc_summary.csv',
            sequencing_run_id + '_auto-cpo_sample_qc_summary.csv',
        ]

        for qc_summary_filename in possible_qc_summary_filenames:
            qc_summary_path = os.path.join(assembly_mode_output_dir, qc_summary_filename)
            if os.path.exists(qc_summary_path):
                if '_sample_' in qc_summary_filename:
                    input_short_reads_qc = parse_sample_qc_summary(qc_summary_path)
                else:
                    input_short_reads_qc = parse_qc_summary(qc_summary_path)
                break
    elif assembly_mode == 'hybrid':
        basic_qc_stats_path = os.path.join(
            assembly_mode_output_dir,
            'basic-nanopore-qc-v0.1-output',
            sequencing_run_id + '_basic_qc_stats.csv'
        )
        if os.path.exists(basic_qc_stats_path):
            input_long_reads_qc = parse_nanopore_basic_qc_stats(basic_qc_stats_path)
        plasmid_screen_fastp_csv_path = os.path.join(
            assembly_mode_output_dir,
            'plasmid-screen-v0.2-output',
            sequencing_run_id + '_fastp.csv'
        )
        if os.path.exists(plasmid_screen_fastp_csv_path):
            input_short_reads_qc = parse_fastp_csv(plasmid_screen_fastp_csv_path)

    input_reads_qc_by_library_id = harmonize_input_reads_qc(input_short_reads_qc, input_long_reads_qc)

    return input_reads_qc_by_library_id


def call_qc_pass_fail(input_reads_qc_by_library_id: dict, qc_criteria: dict, assembly_mode: str) -> dict:
    """
    Determine overall pass/fail status for a library based on QC summary

    :param input_reads_qc_by_library_id: dict of input reads qc summary info, indexed by library id
    :type input_reads_qc_by_library_id: dict
    :return: dict of qc info, with pass/fail status
    :rtype: dict
    """
    min_bases_short = qc_criteria.get('min_total_bases_input_short', None)
    min_bases_long = qc_criteria.get('min_total_bases_input_long', None)
    if not min_bases_short and not min_bases_long:
        for library_id, qc in input_reads_qc_by_library_id.items():
            input_reads_qc_by_library_id[library_id]['total_bases_input_short_pass_fail'] = None
            input_reads_qc_by_library_id[library_id]['total_bases_input_long_pass_fail'] = None
            input_reads_qc_by_library_id[library_id]['overall_pass_fail'] = None
        return input_reads_qc_by_library_id

    if assembly_mode == 'short':
        for library_id, qc in input_reads_qc_by_library_id.items():
            total_bases_input_short = qc.get('total_bases_input_short', None)
            if total_bases_input_short is not None:
                if total_bases_input_short >= min_bases_short:
                    input_reads_qc_by_library_id[library_id]['total_bases_input_short_pass_fail'] = 'PASS'
                else:
                    input_reads_qc_by_library_id[library_id]['total_bases_input_short_pass_fail'] = 'FAIL'
            else:
                input_reads_qc_by_library_id[library_id]['total_bases_input_short_pass_fail'] = None

            input_reads_qc_by_library_id[library_id]['total_bases_input_long_pass_fail'] = '-'
            if input_reads_qc_by_library_id[library_id]['total_bases_input_short_pass_fail'] == 'FAIL':
                input_reads_qc_by_library_id[library_id]['overall_pass_fail'] = 'FAIL'
            else:
                input_reads_qc_by_library_id[library_id]['overall_pass_fail'] = 'PASS'

        return input_reads_qc_by_library_id

    if assembly_mode == 'hybrid':
        for library_id, qc in input_reads_qc_by_library_id.items():
            total_bases_input_short = qc.get('total_bases_input_short', None)
            if total_bases_input_short is not None:
                if total_bases_input_short >= min_bases_short:
                    input_reads_qc_by_library_id[library_id]['total_bases_input_short_pass_fail'] = 'PASS'
                else:
                    input_reads_qc_by_library_id[library_id]['total_bases_input_short_pass_fail'] = 'FAIL'
            else:
                input_reads_qc_by_library_id[library_id]['total_bases_input_short_pass_fail'] = None

        for library_id, qc in input_reads_qc_by_library_id.items():
            total_bases_input_long = qc.get('total_bases_input_long', None)
            if total_bases_input_long is not None:
                if total_bases_input_long >= min_bases_long:
                    input_reads_qc_by_library_id[library_id]['total_bases_input_long_pass_fail'] = 'PASS'
                else:
                    input_reads_qc_by_library_id[library_id]['total_bases_input_long_pass_fail'] = 'FAIL'
            else:
                input_reads_qc_by_library_id[library_id]['total_bases_input_long_pass_fail'] = None

            if input_reads_qc_by_library_id[library_id]['total_bases_input_short_pass_fail'] == 'FAIL' or \
               input_reads_qc_by_library_id[library_id]['total_bases_input_long_pass_fail'] == 'FAIL':
                input_reads_qc_by_library_id[library_id]['overall_pass_fail'] = 'FAIL'
            else:
                input_reads_qc_by_library_id[library_id]['overall_pass_fail'] = 'PASS'

    return input_reads_qc_by_library_id


def parse_mlst_sequence_type(mlst_sequence_type_csv_path: Path) -> dict:
    """
    Parse a mlst sequence type csv file and return a dict of sequence type info

    :param mlst_sequence_type_csv_path: path to mlst sequence type csv file
    :type mlst_sequence_type_csv_path: Path
    :return: dict of sequence type info
    :rtype: dict
    """
    sequence_type = {}
    fields_to_collect = {
        'scheme':        'mlst_scheme',
        'sequence_type': 'mlst_sequence_type',
        'score':         'mlst_score',
    }
    with open(mlst_sequence_type_csv_path, 'r') as f:
        reader = csv.DictReader(f, dialect='unix')
        for row in reader:
            for original_field, output_field in fields_to_collect.items():
                sequence_type[output_field] = row[original_field]

    return sequence_type


def parse_mlst_alleles(mlst_json_path: Path) -> dict:
    """
    Parse a mlst json file and return a dict of mlst alleles

    :param mlst_json_path: path to mlst json file
    :type mlst_json_path: Path
    :return: dict of mlst alleles
    :rtype: dict
    """
    mlst_json = {}
    with open(mlst_json_path, 'r') as f:
        mlst_json = json.load(f)
    k = list(mlst_json.keys())[0]
    v = mlst_json[k]
    alleles = v['alleles']
    if alleles is not None:
        alleles_sorted = dict(sorted(alleles.items()))
    else:
        alleles_sorted = {}

    alleles = {'mlst_alleles': str(alleles_sorted)}

    return alleles


def collect_mlst_results(mlst_output_dir: Path) -> dict:
    """
    Collect mlst results from a directory of mlst output files

    :param mlst_output_dir: path to directory containing mlst output files
    :type mlst_output_dir: Path
    :return: dict of mlst results
    :rtype: dict
    """
    mlst_results_by_sample_id = {}

    sequence_type_by_sample_id = {}
    sequence_type_files = glob.glob(os.path.join(mlst_output_dir, '*', '*_sequence_type.csv'))
    for sequence_type_file in sequence_type_files:
        sample_id = os.path.basename(sequence_type_file).split('_')[0]
        sequence_type = parse_mlst_sequence_type(sequence_type_file)
        sequence_type_by_sample_id[sample_id] = sequence_type

    mlst_results_by_sample_id = sequence_type_by_sample_id.copy()

    mlst_alleles_by_sample_id = {}
    mlst_alleles_files = sequence_type_files = glob.glob(os.path.join(mlst_output_dir, '*', '*_mlst.json'))
    for mlst_alleles_file in mlst_alleles_files:
        sample_id = os.path.basename(mlst_alleles_file).split('_')[0]
        alleles = parse_mlst_alleles(mlst_alleles_file)
        mlst_alleles_by_sample_id[sample_id] = alleles

    for k, v in mlst_alleles_by_sample_id.items():
        mlst_results_by_sample_id[k].update(mlst_alleles_by_sample_id[k])

    mlst_provenance_files = glob.glob(os.path.join(mlst_output_dir, '*', '*_provenance.yml'))
    mlst_provenance_by_sample_id = {}
    for mlst_provenance_file in mlst_provenance_files:
        sample_id = os.path.basename(mlst_provenance_file).split('_')[0]
        parsed_mlst_provenance = parse_mlst_provenance(mlst_provenance_file)
        mlst_provenance_by_sample_id[sample_id] = parsed_mlst_provenance

    for k, v in mlst_provenance_by_sample_id.items():
        mlst_results_by_sample_id[k].update(mlst_provenance_by_sample_id[k])

    return mlst_results_by_sample_id


def parse_top_species(top_species_csv_path: Path) -> dict:
    """
    Parse a top species csv file and return a dict of top species info

    :param top_species_csv_path: path to top species csv file
    :type top_species_csv_path: Path
    :return: dict of top species info
    :rtype: dict
    """
    top_species = {}
    fields_to_collect = {
        'abundance_1_name':                  'species_1_name',
        'abundance_1_ncbi_taxonomy_id':      'species_1_taxid',
        'abundance_1_fraction_total_reads':  'species_1_percent',
        'abundance_2_name':                  'species_2_name',
        'abundance_2_ncbi_taxonomy_id':      'species_2_taxid',
        'abundance_2_fraction_total_reads':  'species_2_percent',
        'unclassified_fraction_total_reads': 'species_unclassified_percent',

    }
    null_output = {x: 'NA' for x in fields_to_collect.values()}

    try:
        with open(top_species_csv_path, 'r') as f:
            reader = csv.DictReader(f, dialect='unix')
            for row in reader:
                for original_field, output_field in fields_to_collect.items():
                    top_species[output_field] = row[original_field]
    except FileNotFoundError as e:
        top_species = null_output

    return top_species


def collect_taxon_abundance_results(taxon_abundance_output_dir: Path) -> dict:
    """
    Collect taxon abundance results from a directory of taxon abundance output files

    :param taxon_abundance_output_dir: path to directory containing taxon abundance output files
    :type taxon_abundance_output_dir: Path
    :return: dict of taxon abundance results
    :rtype: dict
    """
    taxon_abundance_results_by_sample_id = {}
    top_species_files = glob.glob(os.path.join(taxon_abundance_output_dir, '*', '*_S_top_5.csv'))
    top_species_by_sample_id = {}
    for top_species_file in top_species_files:
        sample_id = os.path.basename(top_species_file).split('_')[0]
        top_species = parse_top_species(top_species_file)
        top_species_by_sample_id[sample_id] = top_species

    taxon_abundance_results_by_sample_id = top_species_by_sample_id.copy()

    taxon_abundance_provenance_files = glob.glob(os.path.join(taxon_abundance_output_dir, '*', '*_provenance.yml'))
    taxon_abundance_provenance_by_sample_id = {}
    for taxon_abundance_provenance_file in taxon_abundance_provenance_files:
        sample_id = os.path.basename(taxon_abundance_provenance_file).split('_')[0]
        parsed_taxon_abundance_provenance = parse_taxon_abundance_provenance(taxon_abundance_provenance_file)
        taxon_abundance_provenance_by_sample_id[sample_id] = parsed_taxon_abundance_provenance

    for k, v in taxon_abundance_provenance_by_sample_id.items():
        taxon_abundance_results_by_sample_id[k].update(taxon_abundance_provenance_by_sample_id[k])

    return taxon_abundance_results_by_sample_id


def parse_resistance_gene_report(resistance_gene_report_tsv_path: Path) -> list[dict]:
    """
    Parse a resistance gene report tsv file and return a list of dicts

    :param resistance_gene_report_tsv_path: path to resistance gene report tsv file
    :type resistance_gene_report_tsv_path: Path
    :return: list of dicts
    :rtype: list[dict]
    """
    resistance_genes = []
    fields_to_collect = {
        'resistance_gene_id':                    'resistance_gene_id',
        'resistance_gene_contig_id':             'resistance_gene_contig_id',
        'resistance_gene_contig_size':           'resistance_gene_contig_size',
        'percent_resistance_gene_coverage':      'resistance_gene_percent_coverage',
        'percent_resistance_gene_identity':      'resistance_gene_percent_identity',
        'num_contigs_in_plasmid_reconstruction': 'plasmid_num_contigs',
        'plasmid_reconstruction_size':           'plasmid_size',
        'replicon_types':                        'plasmid_replicon_types',
        'mob_suite_primary_cluster_id':          'plasmid_mob_suite_primary_cluster_id',
        'mob_suite_secondary_cluster_id':        'plasmid_mob_suite_secondary_cluster_id',
        'mash_nearest_neighbor':                 'plasmid_mash_nearest_neighbor',
        'alignment_ref_plasmid':                 'plasmid_alignment_ref',
        'depth_coverage_threshold':              'plasmid_alignment_depth_threshold',
        'percent_ref_plasmid_coverage_above_depth_threshold': 'plasmid_alignment_percent_coverage_above_depth_threshold',
        'num_snps_vs_ref_plasmid':               'plasmid_alignment_num_snps',
    }
    null_output = {x: 'NA' for x in fields_to_collect.values()}

    try:
        with open(resistance_gene_report_tsv_path, 'r') as f:
            reader = csv.DictReader(f, dialect='excel-tab')
            for row in reader:
                gene = {}
                for original_field, output_field in fields_to_collect.items():
                    gene[output_field] = row[original_field]
                contig_localization = None
                if 'chromosome' in row['assembly_file']:
                    contig_localization = 'chromosome'
                elif 'plasmid' in row['assembly_file']:
                    contig_localization = 'plasmid'
                gene['resistance_gene_contig_localization'] = contig_localization
                resistance_genes.append(gene)
    except FileNotFoundError as e:
        resistance_genes = [null_output]

    return resistance_genes


def collect_plasmid_results(plasmid_screen_output_dir: Path) -> dict:
    """
    Collect plasmid results from a directory of plasmid screen output files

    :param plasmid_screen_output_dir: path to directory containing plasmid screen output files
    :type plasmid_screen_output_dir: Path
    :return: dict of plasmid results
    :rtype: dict
    """
    plasmid_results_by_sample_id = {}
    chromosomal_carbapenemases_by_sample_id = {}

    # Although we're collecting plasmid results here, we occasionally see carbapenemases that are
    # assigned to the chromosome. Those are sometimes missed in the 'resistance_gene_report.tsv' files.
    # So we'll collect all any chromosomal carbapenemases here and make sure they're included in the
    # plasmid results.
    abricate_report_files = glob.glob(os.path.join(plasmid_screen_output_dir, '*', '*_abricate.tsv'))
    abricate_ncbi_report_files = glob.glob(os.path.join(plasmid_screen_output_dir, '*', '*_abricate_ncbi.tsv'))
    all_abricate_report_files = abricate_report_files + abricate_ncbi_report_files
    for abricate_report_file in all_abricate_report_files:
        sample_id = os.path.basename(abricate_report_file).split('_')[0]
        abricate_report = parse_abricate_report(abricate_report_file)
        abricate_carbapenemases = [x for x in abricate_report if 'CARBAPENEM' in x['resistance']]
        chromosomal_carbapenemases = [x for x in abricate_carbapenemases if 'chromosome' in x['contig_localization']]
        chromosomal_carbapenemases_by_sample_id[sample_id] = chromosomal_carbapenemases

    resistance_gene_report_files = glob.glob(os.path.join(plasmid_screen_output_dir, '*', '*_resistance_gene_report.tsv'))
    for resistance_gene_report_file in resistance_gene_report_files:
        sample_id = os.path.basename(resistance_gene_report_file).split('_')[0]
        resistance_genes = parse_resistance_gene_report(resistance_gene_report_file)
        plasmid_results_by_sample_id[sample_id] = resistance_genes

    for sample_id, chromosomal_carbapenemases in chromosomal_carbapenemases_by_sample_id.items():
        if sample_id not in plasmid_results_by_sample_id.keys():
            plasmid_results_by_sample_id[sample_id] = []
        for chromosomal_carbapenemase in chromosomal_carbapenemases:
            chromosomal_carbapenemase_plasmid_result_record = {
                'resistance_gene_id': chromosomal_carbapenemase['gene'],
                'resistance_gene_contig_localization': chromosomal_carbapenemase['contig_localization'],
                'resistance_gene_contig_id': chromosomal_carbapenemase['contig_id'],
                'resistance_gene_percent_coverage': chromosomal_carbapenemase['percent_coverage'],
                'resistance_gene_percent_identity': chromosomal_carbapenemase['percent_identity'],
                'plasmid_num_contigs': '-',
                'plasmid_size': '-',
                'plasmid_replicon_types': '-',
                'plasmid_mob_suite_primary_cluster_id': '-',
                'plasmid_mob_suite_secondary_cluster_id': '-',
                'plasmid_mash_nearest_neighbor': '-',
                'plasmid_alignment_ref': '-',
                'plasmid_alignment_depth_threshold': '-',
                'plasmid_alignment_percent_coverage_above_depth_threshold': '-',
                'plasmid_alignment_num_snps': '-',
            }
            carbapenemase_found = False
            for resistance_gene in plasmid_results_by_sample_id[sample_id]:
                conditions_checked = {
                    'resistance_gene_id': resistance_gene['resistance_gene_id'] == chromosomal_carbapenemase_plasmid_result_record['resistance_gene_id'],
                    'resistance_gene_contig_id': resistance_gene['resistance_gene_contig_id'] == chromosomal_carbapenemase_plasmid_result_record['resistance_gene_contig_id'],
                }
                if all(conditions_checked.values()):
                    carbapenemase_found = True
                    break

            if not carbapenemase_found:
                plasmid_results_by_sample_id[sample_id].append(chromosomal_carbapenemase_plasmid_result_record)
        
    return plasmid_results_by_sample_id


def parse_fastp_csv(fastp_csv_path: Path) -> dict:
    """
    Parse a fastp csv file and return a dict of stats

    :param fastp_csv_path: path to fastp csv file
    :type fastp_csv_path: Path
    :return: dict of stats. Keys are: ['read1_mean_length', 'read2_mean_length', 'q30_rate_short']
    :rtype: dict
    """
    fastp_stats = []
    int_fields = [
        'total_reads_before_filtering',
        'total_reads_after_filtering',
        'total_bases_before_filtering',
        'total_bases_after_filtering',
        'read1_mean_length_before_filtering',
        'read1_mean_length_after_filtering',
        'read2_mean_length_before_filtering',
        'read2_mean_length_after_filtering',
        'q20_bases_before_filtering',
        'q20_bases_after_filtering',
        'q30_bases_before_filtering',
        'q30_bases_after_filtering',
    ]

    float_fields = [
        'q20_rate_before_filtering',
        'q20_rate_after_filtering',
        'q30_rate_before_filtering',
        'q30_rate_after_filtering',
    ]
    with open(fastp_csv_path, 'r') as f:
        reader = csv.DictReader(f, dialect='unix')
        for row in reader:
            fastp_stats_record = {}
            for k, v in row.items():
                if k in int_fields:
                    try:
                        fastp_stats_record[k] = int(v)
                    except ValueError:
                        fastp_stats_record[k] = None
                elif k in float_fields:
                    try:
                        fastp_stats_record[k] = float(v)
                    except ValueError:
                        fastp_stats_record[k] = None
                else:
                    fastp_stats_record[k] = v
            fastp_stats.append(fastp_stats_record)

    return fastp_stats


def parse_nanoq_csv(nanoq_csv_path: Path) -> dict:
    """
    Parse a nanoq csv file and return a dict of stats

    :param nanoq_csv_path: path to nanoq csv file
    :type nanoq_csv_path: Path
    :return: dict of stats.
    :rtype: dict
    """
    nanoq_stats = {}
    fields_to_collect = {
    }
    with open(nanoq_csv_path, 'r') as f:
        reader = csv.DictReader(f, dialect='unix')
        for row in reader:
            for original_field, output_field in fields_to_collect.items():
                nanoq_stats[output_field] = row[original_field]

    return nanoq_stats


def parse_quast_csv(quast_csv_path: Path) -> dict:
    """
    Parse a quast csv file and return a dict of stats

    :param quast_csv_path: path to quast csv file
    :type quast_csv_path: Path
    :return: dict of stats. Keys are: ['assembly_total_length', 'assembly_num_contigs', 'assembly_N50', 'assembly_N75']
    :rtype: dict
    """
    quast_stats = {}
    fields_to_collect = {
        'total_length': 'assembly_total_length',
        'num_contigs':  'assembly_num_contigs',
        'assembly_N50': 'assembly_N50',
        'assembly_N75': 'assembly_N75',
    }
    with open(quast_csv_path, 'r') as f:
        reader = csv.DictReader(f, dialect='unix')
        for row in reader:
            for original_field, output_field in fields_to_collect.items():
                quast_stats[output_field] = row[original_field]
    return quast_stats


def parse_abricate_report(abricate_report_tsv_path: Path) -> list[dict]:
    """
    Parse the Abricate report TSV file and return a list of dictionaries.

    :param abricate_report_tsv_path: Path to the Abricate report TSV file.
    :type abricate_report_tsv_path: Path
    :return: List of dictionaries containing the parsed Abricate report.
    :rtype: list[dict]
    """
    parsed_abricate_report = []
    fieldname_translations = {
        'sequence': 'contig_id',
    }
    int_fields = [
        'start',
        'end',
    ]
    float_fields = [
        'percent_coverage',
        'percent_identity',
    ]
    with open(abricate_report_tsv_path, 'r') as f:
        reader = csv.DictReader(f, delimiter='\t')
        for row in reader:
            abricate_report_record = {}
            original_keys = list(row.keys())
            cleaned_keys = [key.strip().lstrip('#').lower().replace('%', 'percent_') for key in row.keys()]

            for idx, cleaned_key in enumerate(cleaned_keys):
                if cleaned_key in fieldname_translations:
                    key = fieldname_translations[cleaned_key]
                else:
                    key = cleaned_key

                original_key = original_keys[idx]

                if key in int_fields:
                    try:
                        abricate_report_record[key] = int(row[original_key])
                    except ValueError:
                        abricate_report_record[key] = None
                elif key in float_fields:
                    try:
                        abricate_report_record[key] = float(row[original_key])
                    except ValueError:
                        abricate_report_record[key] = None
                else:
                    abricate_report_record[key] = row[original_key]

            parsed_abricate_report.append(abricate_report_record)

    for record in parsed_abricate_report:
        filename = record['file']
        if 'chromosome' in filename:
            contig_localization = 'chromosome'
        elif 'plasmid' in filename:
            contig_localization = 'plasmid'
        else:
            contig_localization = None

        record['contig_localization'] = contig_localization

    return parsed_abricate_report
    

def collect_assembly_results(assembly_output_dir, assembler):
    """
    Collect assembly results from a directory of assembly output files

    :param assembly_output_dir: path to directory containing assembly output files
    :type assembly_output_dir: Path
    :return: dict of assembly results
    :rtype: dict
    """
    assembly_results_by_sample_id = {}

    fastp_stats_files = glob.glob(os.path.join(assembly_output_dir, '*', '*_fastp.csv'))
    
    fastp_stats_by_sample_id = {}
    for fastp_stats_file in fastp_stats_files:
        sample_id = os.path.basename(fastp_stats_file).split('_')[0]
        fastp_stats = parse_fastp_csv(fastp_stats_file)
        for fastp_stats_record in fastp_stats:
            if fastp_stats_record['sample_id'] == sample_id:
                fastp_stats_by_sample_id[sample_id] = {
                    'read1_mean_length': fastp_stats_record['read1_mean_length_before_filtering'],
                    'read2_mean_length': fastp_stats_record['read2_mean_length_before_filtering'],
                    'q30_rate_short': fastp_stats_record['q30_rate_before_filtering'],
                }
                break
    
    assembly_results_by_sample_id = fastp_stats_by_sample_id.copy()

    quast_stats_by_sample_id = {}
    quast_stats_files = glob.glob(os.path.join(assembly_output_dir, '*', '*_' + assembler + '*_quast.csv'))
    for quast_stats_file in quast_stats_files:
        sample_id = os.path.basename(quast_stats_file).split('_')[0]
        quast_stats_by_sample_id[sample_id] = parse_quast_csv(quast_stats_file)

    for k, v in assembly_results_by_sample_id.items():
        if k in quast_stats_by_sample_id.keys():
            assembly_results_by_sample_id[k].update(quast_stats_by_sample_id[k])
        else:
            assembly_results_by_sample_id[k].update({
                'assembly_total_length': None,
                'assembly_num_contigs': None,
                'assembly_N50': None,
                'assembly_N75': None,
            })

    assembly_provenance_files = glob.glob(os.path.join(assembly_output_dir, '*', '*_provenance.yml'))
    
    # latest_assembly_provenance_file = assembly_provenance_files[-1]
    
    assembly_provenance_by_sample_id = {}
    for assembly_provenance_file in assembly_provenance_files:
        sample_id = os.path.basename(assembly_provenance_file).split('_')[0]
        parsed_assembly_provenance = parse_assembly_provenance(assembly_provenance_file)
        assembly_provenance_by_sample_id[sample_id] = parsed_assembly_provenance

    for k, v in assembly_results_by_sample_id.items():
        if k in assembly_provenance_by_sample_id.keys():
            assembly_results_by_sample_id[k].update(assembly_provenance_by_sample_id[k])
        else:
            assembly_results_by_sample_id[k].update({
                'assembly_pipeline_name': None,
                'assembly_pipeline_version': None,
            })
    
    return assembly_results_by_sample_id
    

def main(args):
    core_output = []
    plasmid_output = []

    miseq_run_id_regex = r'^[0-9]{6}_M[0-9]{5}_[0-9]{4}_[02]{9}-[A-Z0-9]{5}$'
    nextseq_run_id_regex = r'^[0-9]{6}_VH[0-9]{5}_[0-9]+_[A-Z0-9]{9}$'

    gridion_run_id_regex = r'^[0-9]{8}_[0-9]{4}_X[0-9]_[A-Z0-9]{8}_[a-z0-9]{8}$'
    promethion_run_id_regex = r'^[0-9]{8}_[0-9]{4}_P2S_[0-9]{5}-[0-9]_[A-Z0-9]{8}_[a-z0-9]{8}$'

    sequencing_run_id = os.path.basename(args.analysis_dir.rstrip('/'))

    instrument_type = None
    if re.match(miseq_run_id_regex, sequencing_run_id) or re.match(nextseq_run_id_regex, sequencing_run_id):
        instrument_type = 'illumina'
    elif re.match(gridion_run_id_regex, sequencing_run_id) or re.match(promethion_run_id_regex, sequencing_run_id):
        instrument_type = 'nanopore'

    assembly_output_dirname = None
    assembly_tool_name = None
    assembly_mode = None
    if instrument_type == 'illumina':
        assembly_output_dirname = 'routine-assembly-v0.4-output'
        assembly_tool_name = 'unicycler'
        assembly_mode = 'short'
    elif instrument_type == 'nanopore':
        assembly_output_dirname = 'plasmid-assembly-v0.1-output'
        assembly_tool_name = 'plassembler'
        assembly_mode = 'hybrid'

    assembly_mode_output_dir = os.path.join(args.analysis_dir, assembly_mode)

    input_reads_qc_by_sample_id = {}
    if os.path.exists(assembly_mode_output_dir):
        input_reads_qc_by_sample_id = find_and_parse_input_reads_qc(assembly_mode_output_dir)

    qc_criteria = {
        'min_total_bases_input_short': args.min_total_bases_input_short,
        'min_total_bases_input_long': args.min_total_bases_input_long,
    }

    input_reads_qc_with_pass_fail_by_sample_id = call_qc_pass_fail(input_reads_qc_by_sample_id, qc_criteria, assembly_mode)
    
    core_results_by_sample_id = input_reads_qc_with_pass_fail_by_sample_id.copy()

    for sample_id, core_results in core_results_by_sample_id.items():
        core_results['assembly_tool_name'] = assembly_tool_name
        

    assembly_output_subdir = os.path.join(assembly_mode_output_dir, assembly_output_dirname)
    assembly_results_by_sample_id = collect_assembly_results(assembly_output_subdir, assembly_tool_name)
    for sample_id in assembly_results_by_sample_id.keys():
        assembly_results_by_sample_id[sample_id]['assembly_mode'] = assembly_mode

    core_results_by_sample_id = join_indexed_dicts(core_results_by_sample_id, assembly_results_by_sample_id)

    mlst_output_dirname = 'mlst-nf-v0.1-output'
    mlst_output_subdir = os.path.join(assembly_mode_output_dir, mlst_output_dirname)
    mlst_results_by_sample_id = {}
    if os.path.exists(mlst_output_subdir):
        mlst_results_by_sample_id = collect_mlst_results(mlst_output_subdir)
    else:
        null_mlst_results = {
            "mlst_scheme": None,
            "mlst_sequence_type": None,
            "mlst_score": None,
            "mlst_alleles": None,
            "mlst_pipeline_name": None,
            "mlst_pipeline_version": None,
        }
        for sample_id in core_results_by_sample_id.keys():
            mlst_results_by_sample_id[sample_id] = null_mlst_results

    join_indexed_dicts(core_results_by_sample_id, mlst_results_by_sample_id)

    taxon_abundance_output_dirname = 'taxon-abundance-v0.1-output'
    taxon_abundance_output_subdir = os.path.join(assembly_mode_output_dir, taxon_abundance_output_dirname)
    taxon_abundance_results_by_sample_id = {}
    if os.path.exists(taxon_abundance_output_subdir):
        taxon_abundance_results_by_sample_id = collect_taxon_abundance_results(taxon_abundance_output_subdir)
    else:
        null_taxon_abundance_results = {
            "species_1_name": None,
            "species_1_taxid": None,
            "species_1_percent": None,
            "species_2_name": None,
            "species_2_taxid": None,
            "species_2_percent": None,
            "species_unclassified_percent": None,
            "species_pipeline_name": None,
            "species_pipeline_version": None,
        }
        for sample_id in core_results_by_sample_id.keys():
            taxon_abundance_results_by_sample_id[sample_id] = null_taxon_abundance_results

    core_results_by_sample_id = join_indexed_dicts(core_results_by_sample_id, taxon_abundance_results_by_sample_id)

    plasmid_results_by_sample_id = {}
    plasmid_screen_output_dirname = 'plasmid-screen-v0.2-output'
    plasmid_screen_output_subdir = os.path.join(assembly_mode_output_dir, plasmid_screen_output_dirname)
    plasmid_screen_results_by_sample_id = collect_plasmid_results(plasmid_screen_output_subdir)

    plasmid_results_by_sample_id = plasmid_screen_results_by_sample_id.copy()

    for k, v in core_results_by_sample_id.items():
        v['sequencing_run_id'] = sequencing_run_id
        v['library_id'] = k
        v['assembly_mode'] = assembly_mode
        core_output.append(v)

    output_fieldnames = [
        'sequencing_run_id',
        'library_id',
        'total_bases_input_short',
        'total_bases_input_short_pass_fail',
        'total_bases_input_long',
        'total_bases_input_long_pass_fail',
        'overall_pass_fail',
        'read1_mean_length',
        'read2_mean_length',
        'q30_rate_short',
        'total_reads_long',
        'total_bases_long',
        'read_mean_length_long',
        'read_n50_long',
        'median_quality_long',
        'assembly_total_length',
        'assembly_num_contigs',
        'assembly_N50',
        'assembly_N75',
        'assembly_pipeline_name',
        'assembly_pipeline_version',
        'assembly_tool_name',
        'assembly_tool_version',
        'assembly_mode',
        'mlst_scheme',
        'mlst_sequence_type',
        'mlst_score',
        'mlst_alleles',
        'mlst_pipeline_name',
        'mlst_pipeline_version',
        'species_1_name',
        'species_1_taxid',
        'species_1_percent',
        'species_2_name',
        'species_2_taxid',
        'species_2_percent',
        'species_unclassified_percent',
        'species_pipeline_name',
        'species_pipeline_version',
    ]

    # Make sure that if the overall QC check fails, other outputs are set to None
    for idx, line in enumerate(core_output):
        sequencing_run_id = line['sequencing_run_id']
        library_id = line['library_id']
        total_bases_input_short = line['total_bases_input_short']
        total_bases_input_short_pass_fail = line['total_bases_input_short_pass_fail']
        total_bases_input_long = line['total_bases_input_long']
        total_bases_input_long_pass_fail = line['total_bases_input_long_pass_fail']
        overall_pass_fail = line['overall_pass_fail']
        if overall_pass_fail == 'FAIL':
            line = {k: None for k in line.keys()}
            line['sequencing_run_id'] = sequencing_run_id
            line['library_id'] = library_id
            line['total_bases_input_short'] = total_bases_input_short
            line['total_bases_input_short_pass_fail'] = total_bases_input_short_pass_fail
            line['total_bases_input_long'] = total_bases_input_long
            line['total_bases_input_long_pass_fail'] = total_bases_input_long_pass_fail
            line['overall_pass_fail'] = overall_pass_fail
            core_output[idx] = line
            
    if not args.output:
        output_writer = csv.DictWriter(sys.stdout, fieldnames=output_fieldnames, dialect='excel-tab', quoting=csv.QUOTE_MINIMAL)
        if not args.noheader:
            output_writer.writeheader()
        for o in core_output:
            output_writer.writerow(o)
    else:
        with open(args.output, 'w') as f:
            output_writer = csv.DictWriter(f, fieldnames=output_fieldnames, dialect='excel-tab', quoting=csv.QUOTE_MINIMAL)
            if not args.noheader:
                output_writer.writeheader()
            for o in core_output:
                output_writer.writerow(o)


    for sample_id, core_results in core_results_by_sample_id.items():
        if sample_id in plasmid_results_by_sample_id.keys():
            for plasmid_result in plasmid_results_by_sample_id[sample_id]:
                plasmid_result['total_bases_input_short'] = core_results['total_bases_input_short']
                plasmid_result['total_bases_input_short_pass_fail'] = core_results['total_bases_input_short_pass_fail']
                plasmid_result['total_bases_input_long'] = core_results['total_bases_input_long']
                plasmid_result['total_bases_input_long_pass_fail'] = core_results['total_bases_input_long_pass_fail']
                plasmid_result['overall_pass_fail'] = core_results['overall_pass_fail']
        else:
            plasmid_results_by_sample_id[sample_id] = [
                {
                    'total_bases_input_short': core_results['total_bases_input_short'],
                    'total_bases_input_short_pass_fail': core_results['total_bases_input_short_pass_fail'],
                    'total_bases_input_long': core_results['total_bases_input_long'],
                    'total_bases_input_long_pass_fail': core_results['total_bases_input_long_pass_fail'],
                    'overall_pass_fail': core_results['overall_pass_fail'],
                }
            ]
            
    for sample_id, plasmid_results in plasmid_results_by_sample_id.items():
        for plasmid_result in plasmid_results:
            plasmid_result['sequencing_run_id'] = sequencing_run_id
            plasmid_result['library_id'] = sample_id
            plasmid_result['assembly_mode'] = assembly_mode
            plasmid_result['assembly_tool_name'] = assembly_tool_name
            plasmid_result['assembly_tool_version'] = None
            if sample_id in assembly_results_by_sample_id.keys():
                plasmid_result['assembly_tool_version'] = assembly_results_by_sample_id[k].get('assembly_tool_version', None)
            plasmid_output.append(plasmid_result)

    resistance_gene_output_fieldnames = [
        'sequencing_run_id',
        'library_id',
        'total_bases_input_short',
        'total_bases_input_short_pass_fail',
        'total_bases_input_long',
        'total_bases_input_long_pass_fail',
        'overall_pass_fail',
        'assembly_tool_name',
        'assembly_tool_version',
        'assembly_mode',
        'resistance_gene_id',
        'resistance_gene_contig_id',
        'resistance_gene_contig_localization',
        'resistance_gene_contig_size',
        'resistance_gene_percent_coverage',
        'resistance_gene_percent_identity',
        'plasmid_num_contigs',
        'plasmid_size',
        'plasmid_replicon_types',
        'plasmid_mob_suite_primary_cluster_id',
        'plasmid_mob_suite_secondary_cluster_id',
        'plasmid_mash_nearest_neighbor',
        'plasmid_alignment_ref',
        'plasmid_alignment_depth_threshold',
        'plasmid_alignment_percent_coverage_above_depth_threshold',
        'plasmid_alignment_num_snps',
    ]

    for sample_id, core_results in core_results_by_sample_id.items():
        if sample_id not in plasmid_results_by_sample_id.keys():
            plasmid_results_by_sample_id[sample_id] = [
                {
                    'total_bases_input_short': core_results['total_bases_input_short'],
                    'total_bases_input_short_pass_fail': core_results['total_bases_input_short_pass_fail'],
                    'total_bases_input_long': core_results['total_bases_input_long'],
                    'total_bases_input_long_pass_fail': core_results['total_bases_input_long_pass_fail'],
                    'overall_pass_fail': core_results['overall_pass_fail'],
                }
            ]

    for idx, line in enumerate(plasmid_output):
        sequencing_run_id = line['sequencing_run_id']
        library_id = line['library_id']
        total_bases_input_short = line['total_bases_input_short']
        total_bases_input_short_pass_fail = line['total_bases_input_short_pass_fail']
        total_bases_input_long = line['total_bases_input_long']
        total_bases_input_long_pass_fail = line['total_bases_input_long_pass_fail']
        overall_pass_fail = line['overall_pass_fail']
        # Blank out all result fields if the overall pass/fail is FAIL
        if overall_pass_fail == 'FAIL':
            line = {k: None for k in line.keys()}
            line['sequencing_run_id'] = sequencing_run_id
            line['library_id'] = library_id
            line['total_bases_input_short'] = total_bases_input_short
            line['total_bases_input_short_pass_fail'] = total_bases_input_short_pass_fail
            line['total_bases_input_long'] = total_bases_input_long
            line['total_bases_input_long_pass_fail'] = total_bases_input_long_pass_fail
            line['overall_pass_fail'] = overall_pass_fail
            plasmid_output[idx] = line

    written_plasmid_rows = set()
    if args.plasmid_output:
        with open(args.plasmid_output, 'w') as f:
            plasmid_output_writer = csv.DictWriter(f, fieldnames=resistance_gene_output_fieldnames, dialect='excel-tab', quoting=csv.QUOTE_MINIMAL)
            if not args.noheader:
                plasmid_output_writer.writeheader()
            for o in plasmid_output:
                if o['overall_pass_fail'] != 'FAIL' and 'library_id' in o and 'resistance_gene_id' in o and 'assembly_mode' in o:
                    library_id_resistance_gene_assembly_mode_trio = o['library_id'] + '-' + o['resistance_gene_id'] + '-' + o['assembly_mode']
                else:
                    library_id_resistance_gene_assembly_mode_trio = None
                if library_id_resistance_gene_assembly_mode_trio not in written_plasmid_rows:
                    plasmid_output_writer.writerow(o)
                    written_plasmid_rows.add(library_id_resistance_gene_assembly_mode_trio)
                else:
                    if library_id_resistance_gene_assembly_mode_trio is not None:
                        print("duplicate plasmid output: " + library_id_resistance_gene_assembly_mode_trio, file=sys.stderr)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-d', '--analysis-dir')
    parser.add_argument('--min-total-bases-input-short', type=int, default=250_000_000)
    parser.add_argument('--min-total-bases-input-long', type=int, default=250_000_000)
    parser.add_argument('--noheader', action='store_true')
    parser.add_argument('--output')
    parser.add_argument('--plasmid-output')
    args = parser.parse_args()
    main(args)
