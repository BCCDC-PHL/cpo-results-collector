#!/usr/bin/env python3

import argparse
import csv
import glob
import json
import os
import sys
import yaml


def jdump(x):
    print(json.dumps(x, indent=2))


def parse_provenance(provenance_path):
    with open(provenance_path, 'r') as f:
        try:
            parsed_provenance = yaml.safe_load(f)
        except yaml.YAMLError as e:
            print(e)
            exit(-1)
    for provenance_item in parsed_provenance:
        if 'timestamp_analysis_start' in provenance_item.keys():
            provenance_item['timestamp_analysis_start'] = str(provenance_item['timestamp_analysis_start'])

    return(parsed_provenance)


def parse_assembly_provenance(provenance_path):
    parsed_provenance = parse_provenance(provenance_path)
    assembly_provenance = {}
    for provenance_item in parsed_provenance:
        if 'pipeline_name' in provenance_item:
            assembly_provenance['assembly_pipeline_name'] = provenance_item['pipeline_name']
        if 'pipeline_version' in provenance_item:
            assembly_provenance['assembly_pipeline_version'] = provenance_item['pipeline_version']

    return assembly_provenance


def parse_mlst_provenance(provenance_path):
    parsed_provenance = parse_provenance(provenance_path)
    mlst_provenance = {}
    for provenance_item in parsed_provenance:
        if 'pipeline_name' in provenance_item:
            mlst_provenance['mlst_pipeline_name'] = provenance_item['pipeline_name']
        if 'pipeline_version' in provenance_item:
            mlst_provenance['mlst_pipeline_version'] = provenance_item['pipeline_version']

    return mlst_provenance


def parse_taxon_abundance_provenance(provenance_path):
    parsed_provenance = parse_provenance(provenance_path)
    taxon_abundance_provenance = {}
    for provenance_item in parsed_provenance:
        if 'pipeline_name' in provenance_item:
            taxon_abundance_provenance['species_pipeline_name'] = provenance_item['pipeline_name']
        if 'pipeline_version' in provenance_item:
            taxon_abundance_provenance['species_pipeline_version'] = provenance_item['pipeline_version']

    return taxon_abundance_provenance
            

def parse_mlst_sequence_type(mlst_sequence_type_csv_path):
    sequence_type = {}
    fields_to_collect = {
        'scheme':        'mlst_scheme',
        'sequence_type': 'mlst_sequence_type',
        'score':         'mlst_score',
    }
    with open(mlst_sequence_type_csv_path, 'r') as f:
        reader = csv.DictReader(f, dialect='unix')
        for row in reader:
            for original_field, output_field in fields_to_collect.items():
                sequence_type[output_field] = row[original_field]

    return sequence_type


def parse_mlst_alleles(mlst_json_path):
    mlst_json = {}
    with open(mlst_json_path, 'r') as f:
        mlst_json = json.load(f)
    k = list(mlst_json.keys())[0]
    v = mlst_json[k]
    alleles = v['alleles']
    if alleles is not None:
        alleles_sorted = dict(sorted(alleles.items()))
    else:
        alleles_sorted = {}

    alleles = {'mlst_alleles': str(alleles_sorted)}

    return alleles


def collect_mlst_results(mlst_output_dir):
    mlst_results_by_sample_id = {}

    sequence_type_by_sample_id = {}
    sequence_type_files = glob.glob(os.path.join(mlst_output_dir, '*', '*_sequence_type.csv'))
    for sequence_type_file in sequence_type_files:
        sample_id = os.path.basename(sequence_type_file).split('_')[0]
        sequence_type = parse_mlst_sequence_type(sequence_type_file)
        sequence_type_by_sample_id[sample_id] = sequence_type

    mlst_results_by_sample_id = sequence_type_by_sample_id.copy()

    mlst_alleles_by_sample_id = {}
    mlst_alleles_files = sequence_type_files = glob.glob(os.path.join(mlst_output_dir, '*', '*_mlst.json'))
    for mlst_alleles_file in mlst_alleles_files:
        sample_id = os.path.basename(mlst_alleles_file).split('_')[0]
        alleles = parse_mlst_alleles(mlst_alleles_file)
        mlst_alleles_by_sample_id[sample_id] = alleles

    for k, v in mlst_alleles_by_sample_id.items():
        mlst_results_by_sample_id[k].update(mlst_alleles_by_sample_id[k])

    mlst_provenance_files = glob.glob(os.path.join(mlst_output_dir, '*', '*_provenance.yml'))
    mlst_provenance_by_sample_id = {}
    for mlst_provenance_file in mlst_provenance_files:
        sample_id = os.path.basename(mlst_provenance_file).split('_')[0]
        parsed_mlst_provenance = parse_mlst_provenance(mlst_provenance_file)
        mlst_provenance_by_sample_id[sample_id] = parsed_mlst_provenance

    for k, v in mlst_provenance_by_sample_id.items():
        mlst_results_by_sample_id[k].update(mlst_provenance_by_sample_id[k])

    return mlst_results_by_sample_id


def parse_top_species(top_species_csv_path):
    top_species = {}
    fields_to_collect = {
        'abundance_1_name':                  'species_1_name',
        'abundance_1_ncbi_taxonomy_id':      'species_1_taxid',
        'abundance_1_fraction_total_reads':  'species_1_percent',
        'abundance_2_name':                  'species_2_name',
        'abundance_2_ncbi_taxonomy_id':      'species_2_taxid',
        'abundance_2_fraction_total_reads':  'species_2_percent',
        'unclassified_fraction_total_reads': 'species_unclassified_percent',

    }
    null_output = {x: 'NA' for x in fields_to_collect.values()}

    try:
        with open(top_species_csv_path, 'r') as f:
            reader = csv.DictReader(f, dialect='unix')
            for row in reader:
                for original_field, output_field in fields_to_collect.items():
                    top_species[output_field] = row[original_field]
    except FileNotFoundError as e:
        top_species = null_output

    return top_species


def collect_taxon_abundance_results(taxon_abundance_output_dir):
    taxon_abundance_results_by_sample_id = {}
    top_species_files = glob.glob(os.path.join(taxon_abundance_output_dir, '*', '*_S_top_5.csv'))
    top_species_by_sample_id = {}
    for top_species_file in top_species_files:
        sample_id = os.path.basename(top_species_file).split('_')[0]
        top_species = parse_top_species(top_species_file)
        top_species_by_sample_id[sample_id] = top_species

    taxon_abundance_results_by_sample_id = top_species_by_sample_id.copy()

    taxon_abundance_provenance_files = glob.glob(os.path.join(taxon_abundance_output_dir, '*', '*_provenance.yml'))
    taxon_abundance_provenance_by_sample_id = {}
    for taxon_abundance_provenance_file in taxon_abundance_provenance_files:
        sample_id = os.path.basename(taxon_abundance_provenance_file).split('_')[0]
        parsed_taxon_abundance_provenance = parse_taxon_abundance_provenance(taxon_abundance_provenance_file)
        taxon_abundance_provenance_by_sample_id[sample_id] = parsed_taxon_abundance_provenance

    for k, v in taxon_abundance_provenance_by_sample_id.items():
        taxon_abundance_results_by_sample_id[k].update(taxon_abundance_provenance_by_sample_id[k])
    

    return taxon_abundance_results_by_sample_id


def parse_resistance_gene_report(resistance_gene_report_tsv_path):
    resistance_genes = []
    fields_to_collect = {
        'resistance_gene_id':                    'resistance_gene_id',
        'resistance_gene_contig_id':             'resistance_gene_contig_id',
        'resistance_gene_contig_size':           'resistance_gene_contig_size',
        'percent_resistance_gene_coverage':      'resistance_gene_percent_coverage',
        'percent_resistance_gene_identity':      'resistance_gene_percent_identity',
        'num_contigs_in_plasmid_reconstruction': 'plasmid_num_contigs',
        'plasmid_reconstruction_size':           'plasmid_size',
        'replicon_types':                        'plasmid_replicon_types',
        'mob_suite_primary_cluster_id':          'plasmid_mob_suite_primary_cluster_id',
        'mob_suite_secondary_cluster_id':        'plasmid_mob_suite_secondary_cluster_id',
        'mash_nearest_neighbor':                 'plasmid_mash_nearest_neighbor',
        'alignment_ref_plasmid':                 'plasmid_alignment_ref',
        'depth_coverage_threshold':              'plasmid_alignment_depth_threshold',
        'percent_ref_plasmid_coverage_above_depth_threshold': 'plasmid_alignment_percent_coverage_above_depth_threshold',
        'num_snps_vs_ref_plasmid':               'plasmid_alignment_num_snps',
    }
    null_output = {x: 'NA' for x in fields_to_collect.values()}

    try:
        with open(resistance_gene_report_tsv_path, 'r') as f:
            reader = csv.DictReader(f, dialect='excel-tab')
            for row in reader:
                gene = {}
                for original_field, output_field in fields_to_collect.items():
                    gene[output_field] = row[original_field]
                resistance_genes.append(gene)
    except FileNotFoundError as e:
        resistance_genes = [null_output]

    return resistance_genes


def collect_plasmid_results(plasmid_screen_output_dir):
    plasmid_results_by_sample_id = {}
    resistance_gene_report_files = glob.glob(os.path.join(plasmid_screen_output_dir, '*', '*_resistance_gene_report.tsv'))
    for resistance_gene_report_file in resistance_gene_report_files:
        sample_id = os.path.basename(resistance_gene_report_file).split('_')[0]
        resistance_genes = parse_resistance_gene_report(resistance_gene_report_file)
        plasmid_results_by_sample_id[sample_id] = resistance_genes
    

    return plasmid_results_by_sample_id


def parse_fastp_csv(fastp_csv_path):
    fastp_stats = {}
    fields_to_collect = {
        'total_reads_before_filtering':       'total_reads',
        'total_bases_before_filtering':       'total_bases',
        'read1_mean_length_before_filtering': 'read1_mean_length',
        'read2_mean_length_before_filtering': 'read2_mean_length',
        'q30_rate_before_filtering':          'q30_rate',
    }
    with open(fastp_csv_path, 'r') as f:
        reader = csv.DictReader(f, dialect='unix')
        for row in reader:
            for original_field, output_field in fields_to_collect.items():
                fastp_stats[output_field] = row[original_field]
    return fastp_stats


def parse_quast_csv(quast_csv_path):
    quast_stats = {}
    fields_to_collect = {
        'total_length': 'assembly_total_length',
        'num_contigs':  'assembly_num_contigs',
        'assembly_N50': 'assembly_N50',
        'assembly_N75': 'assembly_N75',
    }
    with open(quast_csv_path, 'r') as f:
        reader = csv.DictReader(f, dialect='unix')
        for row in reader:
            for original_field, output_field in fields_to_collect.items():
                quast_stats[output_field] = row[original_field]
    return quast_stats


def collect_assembly_results(assembly_output_dir, assembler):
    assembly_results_by_sample_id = {}

    fastp_stats_files = glob.glob(os.path.join(assembly_output_dir, '*', '*_fastp.csv'))
    
    fastp_stats_by_sample_id = {}
    for fastp_stats_file in fastp_stats_files:
        sample_id = os.path.basename(fastp_stats_file).split('_')[0]
        fastp_stats_by_sample_id[sample_id] = parse_fastp_csv(fastp_stats_file)
    
    assembly_results_by_sample_id = fastp_stats_by_sample_id.copy()

    quast_stats_by_sample_id = {}
    quast_stats_files = glob.glob(os.path.join(assembly_output_dir, '*', '*_' + assembler + '*_quast.csv'))
    for quast_stats_file in quast_stats_files:
        sample_id = os.path.basename(quast_stats_file).split('_')[0]
        quast_stats_by_sample_id[sample_id] = parse_quast_csv(quast_stats_file)

    for k, v in assembly_results_by_sample_id.items():
        assembly_results_by_sample_id[k].update(quast_stats_by_sample_id[k])

    assembly_provenance_files = glob.glob(os.path.join(assembly_output_dir, '*', '*_provenance.yml'))
    
    # latest_assembly_provenance_file = assembly_provenance_files[-1]
    
    assembly_provenance_by_sample_id = {}
    for assembly_provenance_file in assembly_provenance_files:
        sample_id = os.path.basename(assembly_provenance_file).split('_')[0]
        parsed_assembly_provenance = parse_assembly_provenance(assembly_provenance_file)
        assembly_provenance_by_sample_id[sample_id] = parsed_assembly_provenance

    for k, v in assembly_results_by_sample_id.items():
        assembly_results_by_sample_id[k].update(assembly_provenance_by_sample_id[k])
    
    return assembly_results_by_sample_id
    

def main(args):
    core_output = []
    plasmid_output = []
    assembly_modes = [
        'short',
        'hybrid',
    ]

    sequencing_run_id = os.path.basename(args.analysis_dir)

    for assembly_mode in assembly_modes:
        assembly_mode_output_dir = os.path.join(args.analysis_dir, assembly_mode)

        if os.path.exists(assembly_mode_output_dir):
            assembly_output_dirname = 'routine-assembly-v0.4-output'
            assembly_output_subdir = os.path.join(assembly_mode_output_dir, assembly_output_dirname)
            assembly_results_by_sample_id = collect_assembly_results(assembly_output_subdir, args.assembler)

            core_results_by_sample_id = assembly_results_by_sample_id.copy()

            mlst_output_dirname = 'mlst-nf-v0.1-output'
            mlst_output_subdir = os.path.join(assembly_mode_output_dir, mlst_output_dirname)
            mlst_results_by_sample_id = collect_mlst_results(mlst_output_subdir)

            for k, v in mlst_results_by_sample_id.items():
                core_results_by_sample_id[k].update(mlst_results_by_sample_id[k])

            taxon_abundance_output_dirname = 'taxon-abundance-v0.1-output'
            taxon_abundance_output_subdir = os.path.join(assembly_mode_output_dir, taxon_abundance_output_dirname)
            taxon_abundance_results_by_sample_id = collect_taxon_abundance_results(taxon_abundance_output_subdir)

            for k, v in taxon_abundance_results_by_sample_id.items():
                core_results_by_sample_id[k].update(taxon_abundance_results_by_sample_id[k])

            plasmid_results_by_sample_id = {}
            plasmid_screen_output_dirname = 'plasmid-screen-v0.2-output'
            plasmid_screen_output_subdir = os.path.join(assembly_mode_output_dir, plasmid_screen_output_dirname)
            plasmid_screen_results_by_sample_id = collect_plasmid_results(plasmid_screen_output_subdir)

            plasmid_results_by_sample_id = plasmid_screen_results_by_sample_id.copy()


            for k, v in core_results_by_sample_id.items():
                v['sequencing_run_id'] = sequencing_run_id
                v['library_id'] = k
                v['assembly_tool_name'] = args.assembler
                v['assembly_mode'] = assembly_mode
                core_output.append(v)

            output_fieldnames = [
                'sequencing_run_id',
                'library_id',
                'total_reads',
                'total_bases',
                'read1_mean_length',
                'read2_mean_length',
                'q30_rate',
                'assembly_total_length',
                'assembly_num_contigs',
                'assembly_N50',
                'assembly_N75',
                'assembly_pipeline_name',
                'assembly_pipeline_version',
                'assembly_tool_name',
                'assembly_mode',
                'mlst_scheme',
                'mlst_sequence_type',
                'mlst_score',
                'mlst_alleles',
                'mlst_pipeline_name',
                'mlst_pipeline_version',
                'species_1_name',
                'species_1_taxid',
                'species_1_percent',
                'species_2_name',
                'species_2_taxid',
                'species_2_percent',
                'species_unclassified_percent',
                'species_pipeline_name',
                'species_pipeline_version',
            ]

            if not args.output:
                output_writer = csv.DictWriter(sys.stdout, fieldnames=output_fieldnames, dialect='excel-tab', quoting=csv.QUOTE_MINIMAL)
                if not args.noheader:
                    output_writer.writeheader()
                for o in core_output:
                    output_writer.writerow(o)
            else:
                with open(args.output, 'w') as f:
                    output_writer = csv.DictWriter(f, fieldnames=output_fieldnames, dialect='excel-tab', quoting=csv.QUOTE_MINIMAL)
                    if not args.noheader:
                        output_writer.writeheader()
                    for o in core_output:
                        output_writer.writerow(o)

            for k, vs in plasmid_results_by_sample_id.items():
                for v in vs:
                    v['sequencing_run_id'] = sequencing_run_id
                    v['library_id'] = k
                    v['assembly_tool_name'] = args.assembler
                    v['assembly_mode'] = assembly_mode
                    plasmid_output.append(v)

            plasmid_output_fieldnames = [
                'sequencing_run_id',
                'library_id',
                'assembly_tool_name',
                'assembly_mode',
                'resistance_gene_id',
                'resistance_gene_contig_id',
                'resistance_gene_contig_size',
                'resistance_gene_percent_coverage',
                'resistance_gene_percent_identity',
                'plasmid_num_contigs',
                'plasmid_size',
                'plasmid_replicon_types',
                'plasmid_mob_suite_primary_cluster_id',
                'plasmid_mob_suite_secondary_cluster_id',
                'plasmid_mash_nearest_neighbor',
                'plasmid_alignment_ref',
                'plasmid_alignment_depth_threshold',
                'plasmid_alignment_percent_coverage_above_depth_threshold',
                'plasmid_alignment_num_snps',
            ]

            written_plasmid_rows = set()
            if args.plasmid_output:
                with open(args.plasmid_output, 'w') as f:
                    plasmid_output_writer = csv.DictWriter(f, fieldnames=plasmid_output_fieldnames, dialect='excel-tab', quoting=csv.QUOTE_MINIMAL)
                    if not args.noheader:
                        plasmid_output_writer.writeheader()
                    for o in plasmid_output:
                        library_id_resistance_gene_assembly_mode_trio = o['library_id'] + '-' + o['resistance_gene_id'] + o['assembly_mode']
                        if library_id_resistance_gene_assembly_mode_trio not in written_plasmid_rows:
                            plasmid_output_writer.writerow(o)
                            written_plasmid_rows.add(library_id_resistance_gene_assembly_mode_trio)
                        else:
                            print("duplicate plasmid output: " + library_id_resistance_gene_assembly_mode_trio, file=sys.stderr)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-d', '--analysis-dir')
    parser.add_argument('-a', '--assembler', default='unicycler')
    parser.add_argument('--noheader', action='store_true')
    parser.add_argument('--output')
    parser.add_argument('--plasmid-output')
    args = parser.parse_args()
    main(args)
