#!/usr/bin/env python3

import argparse
import csv
import glob
import json
import os
import re
import sys
import yaml


def jdump(x):
    """
    Just a convenience function to print JSON with indentation
    for debugging purposes.

    :param x: object to print as JSON
    :type x: any
    :return: None
    :rtype: None
    """
    print(json.dumps(x, indent=2))


def join_indexed_dicts(dict_1, dict_2):
    """
    dict_1 is a dict of dicts, where the keys of the outer dict are the index
    dict_2 is a dict of dicts, where the keys of the outer dict are the index
    Returns a dict of dicts, where the keys of the outer dict are the index, and the values are the union of the inner dicts

    :param dict_1: dict of dicts
    :type dict_1: dict[str, dict]
    :param dict_2: dict of dicts
    :type dict_2: dict[str, dict]
    :return: dict of dicts
    :rtype: dict[str, dict]
    """
    joined_dict = {}
    if not dict_1:
        return dict_2
    if not dict_2:
        return dict_1
    dict_1_index = set(dict_1.keys())
    dict_2_keys = list(dict_2.values())[0].keys()

    for k in dict_1_index:
        joined_dict[k] = dict_1[k]
    for k in dict_1_index:
        if k in dict_2.keys():
            for k2 in dict_2_keys:
                joined_dict[k][k2] = dict_2[k][k2]
        else:
            for k2 in dict_2_keys:
                joined_dict[k][k2] = None

    return joined_dict


def parse_provenance(provenance_path):
    """
    Parse a provenance file and return a dictionary.

    :param provenance_path: path to the provenance file
    :type provenance_path: str
    :return: dictionary of provenance items
    :rtype: dict
    """
    with open(provenance_path, 'r') as f:
        try:
            parsed_provenance = yaml.safe_load(f)
        except yaml.YAMLError as e:
            print(e)
            exit(-1)
    for provenance_item in parsed_provenance:
        if 'timestamp_analysis_start' in provenance_item.keys():
            provenance_item['timestamp_analysis_start'] = str(provenance_item['timestamp_analysis_start'])

    return(parsed_provenance)


def parse_assembly_provenance(provenance_path):
    """
    Parse an assembly provenance file and return a dictionary of assembly provenance items

    :param provenance_path: path to the assembly provenance file
    :type provenance_path: str
    :return: dictionary of assembly provenance items
    :rtype: dict
    """
    parsed_provenance = parse_provenance(provenance_path)
    assembly_provenance = {}
    for provenance_item in parsed_provenance:
        if 'pipeline_name' in provenance_item:
            assembly_provenance['assembly_pipeline_name'] = provenance_item['pipeline_name']
        if 'pipeline_version' in provenance_item:
            assembly_provenance['assembly_pipeline_version'] = provenance_item['pipeline_version']

    return assembly_provenance


def parse_mlst_provenance(provenance_path):
    """
    Parse an MLST provenance file and return a dictionary of MLST provenance items

    :param provenance_path: path to the MLST provenance file
    :type provenance_path: str
    :return: dictionary of MLST provenance items
    :rtype: dict
    """
    parsed_provenance = parse_provenance(provenance_path)
    mlst_provenance = {}
    for provenance_item in parsed_provenance:
        if 'pipeline_name' in provenance_item:
            mlst_provenance['mlst_pipeline_name'] = provenance_item['pipeline_name']
        if 'pipeline_version' in provenance_item:
            mlst_provenance['mlst_pipeline_version'] = provenance_item['pipeline_version']

    return mlst_provenance


def parse_taxon_abundance_provenance(provenance_path):
    """
    Parse a taxon abundance provenance file and return a dictionary of taxon abundance provenance items

    :param provenance_path: path to the taxon abundance provenance file
    :type provenance_path: str
    :return: dictionary of taxon abundance provenance items
    :rtype: dict
    """
    parsed_provenance = parse_provenance(provenance_path)
    taxon_abundance_provenance = {}
    for provenance_item in parsed_provenance:
        if 'pipeline_name' in provenance_item:
            taxon_abundance_provenance['species_pipeline_name'] = provenance_item['pipeline_name']
        if 'pipeline_version' in provenance_item:
            taxon_abundance_provenance['species_pipeline_version'] = provenance_item['pipeline_version']

    return taxon_abundance_provenance


def parse_qc_summary(qc_summary_path):
    """
    Parse a QC summary file and return a dictionary of QC items

    :param qc_summary_path: path to the QC summary file
    :type qc_summary_path: str
    :return: dictionary of QC items
    :rtype: dict
    """
    qc_summary = {}
    with open(qc_summary_path, 'r') as f:
        reader = csv.DictReader(f, dialect='unix')
        for row in reader:
            library_id = row['library_id']
            qc = {'library_id': library_id}
            try:
                qc['total_bases_input_short'] = int(row['total_bases_input'])
            except ValueError as e:
                row['total_bases_input'] = None
            if 'total_bases_input_long' in row.keys():
                try:
                    qc['total_bases_input_long'] = int(row['total_bases_input_long'])
                except ValueError as e:
                    qc['total_bases_input_long'] = None
            else:
                qc['total_bases_input_long'] = 0
            if 'total_bases_input_pass_fail' in row.keys():
                qc['total_bases_input_short_pass_fail'] = row['total_bases_input_pass_fail']
            if 'total_bases_input_long_pass_fail' in row.keys():
                qc['total_bases_input_long_pass_fail'] = row['total_bases_input_long_pass_fail']
            else:
                qc['total_bases_input_long_pass_fail'] = None

            if qc['total_bases_input_long_pass_fail'] == None:
                qc['overall_pass_fail'] = qc['total_bases_input_short_pass_fail']
            else:
                qc['overall_pass_fail'] = 'FAIL'
                qc_checks = [qc[field] for field in ['total_bases_input_long_pass_fail', 'total_bases_input_short_pass_fail']]
                if not any([x == 'FAIL' for x in qc_checks]):
                    qc['overall_pass_fail'] = 'WARN'
                if all([x == 'PASS' for x in qc_checks]):
                    qc['overall_pass_fail'] = 'PASS'
            
            qc_summary[library_id] = qc

    return qc_summary


def parse_mlst_sequence_type(mlst_sequence_type_csv_path):
    """
    Parse an MLST sequence type csv file and return a dictionary of MLST sequence type items

    :param mlst_sequence_type_csv_path: path to the MLST sequence type csv file
    :type mlst_sequence_type_csv_path: str
    :return: dictionary of MLST sequence type items
    :rtype: dict
    """
    sequence_type = {}
    fields_to_collect = {
        'scheme':        'mlst_scheme',
        'sequence_type': 'mlst_sequence_type',
        'score':         'mlst_score',
    }
    with open(mlst_sequence_type_csv_path, 'r') as f:
        reader = csv.DictReader(f, dialect='unix')
        for row in reader:
            for original_field, output_field in fields_to_collect.items():
                sequence_type[output_field] = row[original_field]

    return sequence_type


def parse_mlst_alleles(mlst_json_path):
    """
    Parse an MLST alleles json file and return a dictionary of MLST alleles

    :param mlst_json_path: path to the MLST alleles json file
    :type mlst_json_path: str
    :return: dictionary of MLST alleles
    :rtype: dict
    """
    mlst_json = {}
    with open(mlst_json_path, 'r') as f:
        mlst_json = json.load(f)
    k = list(mlst_json.keys())[0]
    v = mlst_json[k]
    alleles = v['alleles']
    if alleles is not None:
        alleles_sorted = dict(sorted(alleles.items()))
    else:
        alleles_sorted = {}

    alleles = {'mlst_alleles': str(alleles_sorted)}

    return alleles


def collect_mlst_results(mlst_output_dir):
    """
    Collect MLST results from an MLST output directory and return a dictionary of MLST results

    :param mlst_output_dir: path to the MLST output directory
    :type mlst_output_dir: str
    :return: dictionary of MLST results
    :rtype: dict
    """
    mlst_results_by_sample_id = {}

    sequence_type_by_sample_id = {}
    sequence_type_files = glob.glob(os.path.join(mlst_output_dir, '*', '*_sequence_type.csv'))
    for sequence_type_file in sequence_type_files:
        sample_id = os.path.basename(sequence_type_file).split('_')[0]
        sequence_type = parse_mlst_sequence_type(sequence_type_file)
        sequence_type_by_sample_id[sample_id] = sequence_type

    mlst_results_by_sample_id = sequence_type_by_sample_id.copy()

    mlst_alleles_by_sample_id = {}
    mlst_alleles_files = sequence_type_files = glob.glob(os.path.join(mlst_output_dir, '*', '*_mlst.json'))
    for mlst_alleles_file in mlst_alleles_files:
        sample_id = os.path.basename(mlst_alleles_file).split('_')[0]
        alleles = parse_mlst_alleles(mlst_alleles_file)
        mlst_alleles_by_sample_id[sample_id] = alleles

    for k, v in mlst_alleles_by_sample_id.items():
        mlst_results_by_sample_id[k].update(mlst_alleles_by_sample_id[k])

    mlst_provenance_files = glob.glob(os.path.join(mlst_output_dir, '*', '*_provenance.yml'))
    mlst_provenance_by_sample_id = {}
    for mlst_provenance_file in mlst_provenance_files:
        sample_id = os.path.basename(mlst_provenance_file).split('_')[0]
        parsed_mlst_provenance = parse_mlst_provenance(mlst_provenance_file)
        mlst_provenance_by_sample_id[sample_id] = parsed_mlst_provenance

    for k, v in mlst_provenance_by_sample_id.items():
        mlst_results_by_sample_id[k].update(mlst_provenance_by_sample_id[k])

    return mlst_results_by_sample_id


def parse_top_species(top_species_csv_path):
    """
    Parse a top species csv file and return a dictionary of top species

    :param top_species_csv_path: path to the top species csv file
    :type top_species_csv_path: str
    :return: dictionary of top species
    :rtype: dict
    """
    top_species = {}
    fields_to_collect = {
        'abundance_1_name':                  'species_1_name',
        'abundance_1_ncbi_taxonomy_id':      'species_1_taxid',
        'abundance_1_fraction_total_reads':  'species_1_percent',
        'abundance_2_name':                  'species_2_name',
        'abundance_2_ncbi_taxonomy_id':      'species_2_taxid',
        'abundance_2_fraction_total_reads':  'species_2_percent',
        'unclassified_fraction_total_reads': 'species_unclassified_percent',

    }
    null_output = {x: 'NA' for x in fields_to_collect.values()}

    try:
        with open(top_species_csv_path, 'r') as f:
            reader = csv.DictReader(f, dialect='unix')
            for row in reader:
                for original_field, output_field in fields_to_collect.items():
                    top_species[output_field] = row[original_field]
    except FileNotFoundError as e:
        top_species = null_output

    return top_species


def collect_taxon_abundance_results(taxon_abundance_output_dir):
    """
    Collect taxon abundance results from a taxon abundance output directory and return a dictionary of taxon abundance results

    :param taxon_abundance_output_dir: path to the taxon abundance output directory
    :type taxon_abundance_output_dir: str
    :return: dictionary of taxon abundance results
    :rtype: dict
    """
    taxon_abundance_results_by_sample_id = {}
    top_species_files = glob.glob(os.path.join(taxon_abundance_output_dir, '*', '*_S_top_5.csv'))
    top_species_by_sample_id = {}
    for top_species_file in top_species_files:
        sample_id = os.path.basename(top_species_file).split('_')[0]
        top_species = parse_top_species(top_species_file)
        top_species_by_sample_id[sample_id] = top_species

    taxon_abundance_results_by_sample_id = top_species_by_sample_id.copy()

    taxon_abundance_provenance_files = glob.glob(os.path.join(taxon_abundance_output_dir, '*', '*_provenance.yml'))
    taxon_abundance_provenance_by_sample_id = {}
    for taxon_abundance_provenance_file in taxon_abundance_provenance_files:
        sample_id = os.path.basename(taxon_abundance_provenance_file).split('_')[0]
        parsed_taxon_abundance_provenance = parse_taxon_abundance_provenance(taxon_abundance_provenance_file)
        taxon_abundance_provenance_by_sample_id[sample_id] = parsed_taxon_abundance_provenance

    for k, v in taxon_abundance_provenance_by_sample_id.items():
        taxon_abundance_results_by_sample_id[k].update(taxon_abundance_provenance_by_sample_id[k])

    return taxon_abundance_results_by_sample_id


def parse_resistance_gene_report(resistance_gene_report_tsv_path):
    """
    Parse a resistance gene report tsv file and return a list of resistance genes

    :param resistance_gene_report_tsv_path: path to the resistance gene report tsv file
    :type resistance_gene_report_tsv_path: str
    :return: list of resistance genes
    :rtype: list[dict]
    """
    resistance_genes = []
    fields_to_collect = {
        'resistance_gene_id':                    'resistance_gene_id',
        'resistance_gene_contig_id':             'resistance_gene_contig_id',
        'resistance_gene_contig_size':           'resistance_gene_contig_size',
        'percent_resistance_gene_coverage':      'resistance_gene_percent_coverage',
        'percent_resistance_gene_identity':      'resistance_gene_percent_identity',
        'num_contigs_in_plasmid_reconstruction': 'plasmid_num_contigs',
        'plasmid_reconstruction_size':           'plasmid_size',
        'replicon_types':                        'plasmid_replicon_types',
        'mob_suite_primary_cluster_id':          'plasmid_mob_suite_primary_cluster_id',
        'mob_suite_secondary_cluster_id':        'plasmid_mob_suite_secondary_cluster_id',
        'mash_nearest_neighbor':                 'plasmid_mash_nearest_neighbor',
        'alignment_ref_plasmid':                 'plasmid_alignment_ref',
        'depth_coverage_threshold':              'plasmid_alignment_depth_threshold',
        'percent_ref_plasmid_coverage_above_depth_threshold': 'plasmid_alignment_percent_coverage_above_depth_threshold',
        'num_snps_vs_ref_plasmid':               'plasmid_alignment_num_snps',
    }
    null_output = {x: 'NA' for x in fields_to_collect.values()}

    try:
        with open(resistance_gene_report_tsv_path, 'r') as f:
            reader = csv.DictReader(f, dialect='excel-tab')
            for row in reader:
                gene = {}
                for original_field, output_field in fields_to_collect.items():
                    if original_field == 'resistance_gene_contig_id':
                        if '=' in row[original_field]:
                            contig_id_split = row[original_field].split('=')
                            gene[output_field] = '_'.join(contig_id_split[0].split('_')[:-1])
                        else:
                            gene[output_field] = row[original_field]
                    else:
                        gene[output_field] = row[original_field]
                resistance_genes.append(gene)
    except FileNotFoundError as e:
        resistance_genes = [null_output]

    return resistance_genes


def parse_abricate_report(abricate_report_tsv_path):
    """
    Parse an abricate report tsv file and return a list of resistance genes

    :param abricate_report_tsv_path: path to the abricate report tsv file
    :type abricate_report_tsv_path: str
    :return: list of resistance genes
    :rtype: list[dict]
    """
    abricate_report = []
    fieldname_translation = {
        "#FILE": "file",
        "SEQUENCE": "resistance_gene_contig_id",
        "START": "resistance_gene_start",
        "END": "resistance_gene_end",
        "STRAND": "resistance_gene_strand",
        "GENE": "resistance_gene_id",
        "%COVERAGE": "resistance_gene_percent_coverage",
        "%IDENTITY": "resistance_gene_percent_identity",
        "DATABASE": "resistance_gene_database",
        "ACCESSION": "database_accession",
        "PRODUCT": "resistance_gene_product",
        "RESISTANCE": "resistance_class",
    }
    int_fields = ['resistance_gene_start', 'resistance_gene_end']
    float_fields = ['resistance_gene_percent_coverage', 'resistance_gene_percent_identity']

    with open(abricate_report_tsv_path, 'r') as f:
        reader = csv.DictReader(f, dialect='excel-tab')
        for row in reader:
            parsed_row = {}
            for original_field, output_field in fieldname_translation.items():
                if output_field == 'resistance_gene_contig_id':
                    if '=' in row[original_field]:
                        contig_id_split = row[original_field].split('=')
                        parsed_row[output_field] = '_'.join(contig_id_split[0].split('_')[:-1])
                    else:
                        parsed_row[output_field] = row[original_field]
                else:
                    parsed_row[output_field] = row[original_field]
            for field in int_fields:
                try:
                    parsed_row[field] = int(parsed_row[field])
                except ValueError as e:
                    parsed_row[field] = None
            for field in float_fields:
                try:
                    parsed_row[field] = float(parsed_row[field])
                except ValueError as e:
                    parsed_row[field] = None
            
            abricate_report.append(parsed_row)

    return abricate_report


def collect_plasmid_screen_results(plasmid_screen_output_dir):
    """
    Collect plasmid results from a plasmid screen output directory and return a dictionary of plasmid results

    :param plasmid_screen_output_dir: path to the plasmid screen output directory
    :type plasmid_screen_output_dir: str
    :return: dictionary of plasmid results
    :rtype: dict
    """
    plasmid_results_by_sample_id = {}
    resistance_gene_report_files = glob.glob(os.path.join(plasmid_screen_output_dir, '*', '*_resistance_gene_report.tsv'))
    abricate_report_files = glob.glob(os.path.join(plasmid_screen_output_dir, '*', '*_abricate.tsv'))
    
    for resistance_gene_report_file in resistance_gene_report_files:
        sample_id = os.path.basename(resistance_gene_report_file).split('_')[0]
        resistance_genes = parse_resistance_gene_report(resistance_gene_report_file)
        plasmid_results_by_sample_id[sample_id] = resistance_genes

    for abricate_report_file in abricate_report_files:
        sample_id = os.path.basename(abricate_report_file).split('_')[0]
        abricate_report = parse_abricate_report(abricate_report_file)
        abricate_carbapenemases = [x for x in abricate_report if x['resistance_class'] == 'CARBAPENEM']
        chromosome_carbapenemases = [x for x in abricate_carbapenemases if 'chromosome' in x['file']]
        

    return plasmid_results_by_sample_id


def parse_fastp_csv(fastp_csv_path):
    """
    Parse a fastp csv file and return a dict of stats

    :param fastp_csv_path: path to fastp csv file
    :return: dict of stats. Keys are: ['read1_mean_length', 'read2_mean_length', 'q30_rate_short']
    :rtype: dict
    """
    fastp_stats = {}
    fields_to_collect = {
        'read1_mean_length_before_filtering': 'read1_mean_length',
        'read2_mean_length_before_filtering': 'read2_mean_length',
        'q30_rate_before_filtering':          'q30_rate_short',
    }
    with open(fastp_csv_path, 'r') as f:
        reader = csv.DictReader(f, dialect='unix')
        for row in reader:
            for original_field, output_field in fields_to_collect.items():
                fastp_stats[output_field] = row[original_field]

    return fastp_stats


def parse_nanoq_csv(nanoq_csv_path):
    """
    Parse a nanoq csv file and return a dict of stats

    :param nanoq_csv_path: path to nanoq csv file
    :return: dict of stats.
    :rtype: dict
    """
    nanoq_stats = {}
    fields_to_collect = {
    }
    with open(nanoq_csv_path, 'r') as f:
        reader = csv.DictReader(f, dialect='unix')
        for row in reader:
            for original_field, output_field in fields_to_collect.items():
                nanoq_stats[output_field] = row[original_field]

    return nanoq_stats


def parse_quast_csv(quast_csv_path):
    """
    Parse a quast csv file and return a dict of stats

    :param quast_csv_path: path to quast csv file
    :return: dict of stats. Keys are: ['assembly_total_length', 'assembly_num_contigs', 'assembly_N50', 'assembly_N75']
    :rtype: dict
    """
    quast_stats = {}
    fields_to_collect = {
        'total_length': 'assembly_total_length',
        'num_contigs':  'assembly_num_contigs',
        'assembly_N50': 'assembly_N50',
        'assembly_N75': 'assembly_N75',
    }
    with open(quast_csv_path, 'r') as f:
        reader = csv.DictReader(f, dialect='unix')
        for row in reader:
            for original_field, output_field in fields_to_collect.items():
                quast_stats[output_field] = row[original_field]
    return quast_stats


def collect_assembly_results(assembly_output_dir, assembler):
    """
    Collect assembly results from an assembly output directory and return a dictionary of assembly results

    :param assembly_output_dir: path to the assembly output directory
    :type assembly_output_dir: str
    :param assembler: name of the assembler (e.g. 'unicycler', 'dragonflye')
    :type assembler: str
    :return: dictionary of assembly results
    :rtype: dict
    """
    assembly_results_by_sample_id = {}

    fastp_stats_files = glob.glob(os.path.join(assembly_output_dir, '*', '*_fastp.csv'))
    
    fastp_stats_by_sample_id = {}
    for fastp_stats_file in fastp_stats_files:
        sample_id = os.path.basename(fastp_stats_file).split('_')[0]
        fastp_stats_by_sample_id[sample_id] = parse_fastp_csv(fastp_stats_file)

    assembly_results_by_sample_id = fastp_stats_by_sample_id.copy()

    quast_stats_by_sample_id = {}
    quast_stats_files = glob.glob(os.path.join(assembly_output_dir, '*', '*_' + assembler + '*_quast.csv'))
    for quast_stats_file in quast_stats_files:
        sample_id = os.path.basename(quast_stats_file).split('_')[0]
        quast_stats_by_sample_id[sample_id] = parse_quast_csv(quast_stats_file)

    for k, v in assembly_results_by_sample_id.items():
        if k in quast_stats_by_sample_id.keys():
            assembly_results_by_sample_id[k].update(quast_stats_by_sample_id[k])
        else:
            assembly_results_by_sample_id[k].update({
                'assembly_total_length': None,
                'assembly_num_contigs': None,
                'assembly_N50': None,
                'assembly_N75': None,
            })

    assembly_provenance_files = glob.glob(os.path.join(assembly_output_dir, '*', '*_provenance.yml'))
    
    assembly_provenance_by_sample_id = {}
    for assembly_provenance_file in assembly_provenance_files:
        sample_id = os.path.basename(assembly_provenance_file).split('_')[0]
        parsed_assembly_provenance = parse_assembly_provenance(assembly_provenance_file)
        assembly_provenance_by_sample_id[sample_id] = parsed_assembly_provenance

    for k, v in assembly_results_by_sample_id.items():
        if k in assembly_provenance_by_sample_id.keys():
            assembly_results_by_sample_id[k].update(assembly_provenance_by_sample_id[k])
        else:
            assembly_results_by_sample_id[k].update({
                'assembly_pipeline_name': None,
                'assembly_pipeline_version': None,
            })
    
    return assembly_results_by_sample_id


def collect_core_results(analysis_dir, assembly_mode):
    """
    Collect core results from an analysis directory and return a dictionary of core results

    :param analysis_dir: path to the analysis directory
    :type analysis_dir: str
    :param assembly_mode: assembly mode ('short', 'long', 'hybrid')
    :type assembly_mode: str
    :return: dictionary of core results
    :rtype: dict
    """
    core_results = []
    core_results_by_sample_id = {}

    sequencing_run_id = os.path.basename(analysis_dir)
    assembly_mode_output_dir = os.path.join(analysis_dir, assembly_mode)

    if assembly_mode == 'short':
        assembly_output_dirname = 'routine-assembly-v0.4-output'
        assembly_tool_name = 'unicycler'

    elif assembly_mode == 'hybrid':
        assembly_output_dirname = 'dragonflye-nf-v0.1-output'
        assembly_tool_name = 'dragonflye'

    # Haven't implemented long-read-only assembly yet
    else:
        return core_results

    #
    # Collect QC Summary
    #
    qc_summary = {}
    if os.path.exists(assembly_mode_output_dir):
        qc_summary_path = os.path.join(assembly_mode_output_dir, sequencing_run_id + '_auto-cpo_qc_summary.csv')
        if os.path.exists(qc_summary_path):
            qc_summary = parse_qc_summary(qc_summary_path)

    core_results_by_sample_id = qc_summary.copy()

    #
    # Collect Assembly Results
    #
    assembly_output_subdir = os.path.join(assembly_mode_output_dir, assembly_output_dirname)
    assembly_results_by_sample_id = collect_assembly_results(assembly_output_subdir, assembly_tool_name)
    for sample_id in assembly_results_by_sample_id.keys():
        assembly_results_by_sample_id[sample_id]['assembly_mode'] = assembly_mode

    core_results_by_sample_id = join_indexed_dicts(core_results_by_sample_id, assembly_results_by_sample_id)

    #
    # Collect MLST Results
    #
    mlst_output_dirname = 'mlst-nf-v0.1-output'
    mlst_output_subdir = os.path.join(assembly_mode_output_dir, mlst_output_dirname)
    mlst_results_by_sample_id = {}
    if os.path.exists(mlst_output_subdir):
        mlst_results_by_sample_id = collect_mlst_results(mlst_output_subdir)
    else:
        null_mlst_results = {
            "mlst_scheme": None,
            "mlst_sequence_type": None,
            "mlst_score": None,
            "mlst_alleles": None,
            "mlst_pipeline_name": None,
            "mlst_pipeline_version": None,
        }
        for sample_id in core_results_by_sample_id.keys():
            mlst_results_by_sample_id[sample_id] = null_mlst_results

    join_indexed_dicts(core_results_by_sample_id, mlst_results_by_sample_id)

    taxon_abundance_output_dirname = 'taxon-abundance-v0.1-output'
    taxon_abundance_output_subdir = os.path.join(assembly_mode_output_dir, taxon_abundance_output_dirname)
    taxon_abundance_results_by_sample_id = {}
    if os.path.exists(taxon_abundance_output_subdir):
        taxon_abundance_results_by_sample_id = collect_taxon_abundance_results(taxon_abundance_output_subdir)
    else:
        null_taxon_abundance_results = {
            "species_1_name": None,
            "species_1_taxid": None,
            "species_1_percent": None,
            "species_2_name": None,
            "species_2_taxid": None,
            "species_2_percent": None,
            "species_unclassified_percent": None,
            "species_pipeline_name": None,
            "species_pipeline_version": None,
        }
        for sample_id in core_results_by_sample_id.keys():
            taxon_abundance_results_by_sample_id[sample_id] = null_taxon_abundance_results

    core_results_by_sample_id = join_indexed_dicts(core_results_by_sample_id, taxon_abundance_results_by_sample_id)

    for k, v in core_results_by_sample_id.items():
        v['sequencing_run_id'] = sequencing_run_id
        v['library_id'] = k
        v['assembly_tool_name'] = assembly_tool_name
        v['assembly_mode'] = assembly_mode
        core_results.append(v)

    # Make sure that if the overall QC check fails, other outputs are set to None
    for idx, line in enumerate(core_results):
        sequencing_run_id = line['sequencing_run_id']
        library_id = line['library_id']
        total_bases_input_short = line.get('total_bases_input_short', None)
        total_bases_input_short_pass_fail = line.get('total_bases_input_short_pass_fail', None)
        total_bases_input_long = line.get('total_bases_input_long', None)
        total_bases_input_long_pass_fail = line.get('total_bases_input_long_pass_fail', None)
        overall_pass_fail = line.get('overall_pass_fail', None)
        if overall_pass_fail == 'FAIL':
            line = {k: None for k in line.keys()}
            line['sequencing_run_id'] = sequencing_run_id
            line['library_id'] = library_id
            line['total_bases_input_short'] = total_bases_input_short
            line['total_bases_input_short_pass_fail'] = total_bases_input_short_pass_fail
            line['total_bases_input_long'] = total_bases_input_long
            line['total_bases_input_long_pass_fail'] = total_bases_input_long_pass_fail
            line['overall_pass_fail'] = overall_pass_fail
            core_results[idx] = line

    return core_results
    

def collect_plasmid_results(analysis_dir, assembly_mode):
    """
    Collect plasmid results from an analysis directory and a list of plasmid results

    :param analysis_dir: path to the analysis directory
    :type analysis_dir: str
    :param assembly_mode: assembly mode ('short', 'long', 'hybrid')
    :type assembly_mode: str
    :return: list of plasmid results
    :rtype: list[dict]
    """
    plasmid_results = []
    plasmid_results_by_sample_id = {}

    sequencing_run_id = os.path.basename(analysis_dir)
    if assembly_mode == 'short':
        assembly_tool_name = 'unicycler'
    elif assembly_mode == 'hybrid':
        assembly_tool_name = 'dragonflye'
    assembly_mode_output_dir = os.path.join(analysis_dir, assembly_mode)
    plasmid_screen_output_dirname = 'plasmid-screen-v0.2-output'
    plasmid_screen_output_subdir = os.path.join(assembly_mode_output_dir, plasmid_screen_output_dirname)
    plasmid_screen_results_by_sample_id = collect_plasmid_screen_results(plasmid_screen_output_subdir)

    plasmid_results_by_sample_id = plasmid_screen_results_by_sample_id.copy()

    for k, vs in plasmid_results_by_sample_id.items():
        for v in vs:
            v['sequencing_run_id'] = sequencing_run_id
            v['library_id'] = k
            v['assembly_tool_name'] = assembly_tool_name
            v['assembly_mode'] = assembly_mode
            plasmid_results.append(v)

    return plasmid_results


def add_qc_summary_to_plasmid_results(core_results, plasmid_results):
    """
    Add QC summary fields to plasmid results

    :param core_results: core results
    :type core_results: list[dict]
    :param plasmid_results: plasmid results
    :type plasmid_results: list[dict]
    :return: list of plasmid results with QC summary fields added
    :rtype: list[dict]
    """

    plasmid_output = []
    core_results_by_sample_id = {x['library_id']: x for x in core_results}
    plasmid_results_by_sample_id = {}
    for plasmid_result in plasmid_results:
        library_id = plasmid_result['library_id']
        if library_id in plasmid_results_by_sample_id.keys():
            plasmid_results_by_sample_id[library_id].append(plasmid_result)
        else:
            plasmid_results_by_sample_id[library_id] = [plasmid_result]

    for k, v in core_results_by_sample_id.items():
        if k in plasmid_results_by_sample_id.keys():
            for plasmid_result in plasmid_results_by_sample_id[k]:
                plasmid_result['total_bases_input_short'] = v.get('total_bases_input_short', None)
                plasmid_result['total_bases_input_short_pass_fail'] = v.get('total_bases_input_short_pass_fail', None)
                plasmid_result['total_bases_input_long'] = v.get('total_bases_input_long', None)
                plasmid_result['total_bases_input_long_pass_fail'] = v.get('total_bases_input_long_pass_fail', None)
                plasmid_result['overall_pass_fail'] = v.get('overall_pass_fail', None)
        else:
            plasmid_results_by_sample_id[k] = [
                {
                    'total_bases_input_short': v.get('total_bases_input_short', None),
                    'total_bases_input_short_pass_fail': v.get('total_bases_input_short_pass_fail', None),
                    'total_bases_input_long': v.get('total_bases_input_long', None),
                    'total_bases_input_long_pass_fail': v.get('total_bases_input_long_pass_fail', None),
                    'overall_pass_fail': v.get('overall_pass_fail', None),
                }
            ]

    for sample_id, plasmid_results in plasmid_results_by_sample_id.items():
        for plasmid_result in plasmid_results:
            plasmid_output.append(plasmid_result)

    return plasmid_output


def main(args):
    if not os.path.exists(args.analysis_dir):
        print(f"Analysis directory not found: {args.analysis_dir}")
        exit(-1)

    core_output = []
    plasmid_output = []

    miseq_run_id_regex = r'^[0-9]{6}_M[0-9]{5}_[0-9]{4}_[02]{9}-[A-Z0-9]{5}$'
    nextseq_run_id_regex = r'^[0-9]{6}_VH[0-9]{5}_[0-9]+_[A-Z0-9]{9}$'

    gridion_run_id_regex = r'^[0-9]{8}_[0-9]{4}_X[0-9]_[A-Z0-9]{8}_[a-z0-9]{8}$'
    promethion_run_id_regex = r'^[0-9]{8}_[0-9]{4}_P2S_[0-9]{5}-[0-9]_[A-Z0-9]{8}_[a-z0-9]{8}$'

    sequencing_run_id = os.path.basename(args.analysis_dir.rstrip('/'))

    instrument_type = None
    if re.match(miseq_run_id_regex, sequencing_run_id) or re.match(nextseq_run_id_regex, sequencing_run_id):
        instrument_type = 'illumina'
    elif re.match(gridion_run_id_regex, sequencing_run_id) or re.match(promethion_run_id_regex, sequencing_run_id):
        instrument_type = 'nanopore'
    else:
        print(f"Could not determine instrument type for sequencing run {sequencing_run_id}")
        exit(-1)

    valid_assembly_modes = ['short', 'long', 'hybrid']
    assembly_modes = []
    analysis_dir_subdirs = os.listdir(args.analysis_dir)
    for subdir in analysis_dir_subdirs:
        if subdir in valid_assembly_modes:
            assembly_modes.append(subdir)

    for assembly_mode in assembly_modes:
        core_output += collect_core_results(args.analysis_dir, assembly_mode)
        plasmid_output += collect_plasmid_results(args.analysis_dir, assembly_mode)

    core_output_fieldnames = [
        'sequencing_run_id',
        'library_id',
        'total_bases_input_short',
        'total_bases_input_short_pass_fail',
        'total_bases_input_long',
        'total_bases_input_long_pass_fail',
        'overall_pass_fail',
        'read1_mean_length',
        'read2_mean_length',
        'q30_rate_short',
        'total_reads_long',
        'total_bases_long',
        'read_mean_length_long',
        'read_n50_long',
        'median_quality_long',
        'assembly_total_length',
        'assembly_num_contigs',
        'assembly_N50',
        'assembly_N75',
        'assembly_pipeline_name',
        'assembly_pipeline_version',
        'assembly_tool_name',
        'assembly_mode',
        'mlst_scheme',
        'mlst_sequence_type',
        'mlst_score',
        'mlst_alleles',
        'mlst_pipeline_name',
        'mlst_pipeline_version',
        'species_1_name',
        'species_1_taxid',
        'species_1_percent',
        'species_2_name',
        'species_2_taxid',
        'species_2_percent',
        'species_unclassified_percent',
        'species_pipeline_name',
        'species_pipeline_version',
    ]

    if not args.output:
        output_writer = csv.DictWriter(sys.stdout, fieldnames=core_output_fieldnames, dialect='unix', quoting=csv.QUOTE_MINIMAL, extrasaction='ignore')
        if not args.noheader:
            output_writer.writeheader()
        for o in core_output:
            output_writer.writerow(o)
    else:
        with open(args.output, 'w') as f:
            output_writer = csv.DictWriter(f, fieldnames=core_output_fieldnames, dialect='unix', quoting=csv.QUOTE_MINIMAL, extrasaction='ignore')
            if not args.noheader:
                output_writer.writeheader()
            for o in core_output:
                output_writer.writerow(o)

    plasmid_output = add_qc_summary_to_plasmid_results(core_output, plasmid_output)

    plasmid_output_fieldnames = [
        'sequencing_run_id',
        'library_id',
        'total_bases_input_short',
        'total_bases_input_short_pass_fail',
        'total_bases_input_long',
        'total_bases_input_long_pass_fail',
        'overall_pass_fail',
        'assembly_tool_name',
        'assembly_mode',
        'resistance_gene_id',
        'resistance_gene_contig_id',
        'resistance_gene_contig_size',
        'resistance_gene_percent_coverage',
        'resistance_gene_percent_identity',
        'plasmid_num_contigs',
        'plasmid_size',
        'plasmid_replicon_types',
        'plasmid_mob_suite_primary_cluster_id',
        'plasmid_mob_suite_secondary_cluster_id',
        'plasmid_mash_nearest_neighbor',
        'plasmid_alignment_ref',
        'plasmid_alignment_depth_threshold',
        'plasmid_alignment_percent_coverage_above_depth_threshold',
        'plasmid_alignment_num_snps',
    ]

    for idx, line in enumerate(plasmid_output):
        sequencing_run_id = line['sequencing_run_id']
        library_id = line['library_id']
        total_bases_input_short = line.get('total_bases_input_short', None)
        total_bases_input_short_pass_fail = line.get('total_bases_input_short_pass_fail', None)
        total_bases_input_long = line.get('total_bases_input_long', None)
        total_bases_input_long_pass_fail = line.get('total_bases_input_long_pass_fail', None)
        overall_pass_fail = line.get('overall_pass_fail', None)
        if overall_pass_fail == 'FAIL':
            line = {k: None for k in line.keys()}
            line['sequencing_run_id'] = sequencing_run_id
            line['library_id'] = library_id
            line['total_bases_input_short'] = total_bases_input_short
            line['total_bases_input_short_pass_fail'] = total_bases_input_short_pass_fail
            line['total_bases_input_long'] = total_bases_input_long
            line['total_bases_input_long_pass_fail'] = total_bases_input_long_pass_fail
            line['overall_pass_fail'] = overall_pass_fail
            plasmid_output[idx] = line

    written_plasmid_rows = set()
    if args.plasmid_output:
        with open(args.plasmid_output, 'w') as f:
            plasmid_output_writer = csv.DictWriter(f, fieldnames=plasmid_output_fieldnames, dialect='unix', quoting=csv.QUOTE_MINIMAL, extrasaction='ignore')
            if not args.noheader:
                plasmid_output_writer.writeheader()
            for o in plasmid_output:
                if o['overall_pass_fail'] != 'FAIL' and 'library_id' in o and 'resistance_gene_id' in o and 'assembly_mode' in o:
                    library_id_resistance_gene_assembly_mode_trio = o['library_id'] + '-' + o['resistance_gene_id'] + '-' + o['assembly_mode']
                else:
                    library_id_resistance_gene_assembly_mode_trio = None
                if library_id_resistance_gene_assembly_mode_trio not in written_plasmid_rows:
                    plasmid_output_writer.writerow(o)
                    written_plasmid_rows.add(library_id_resistance_gene_assembly_mode_trio)
                else:
                    if library_id_resistance_gene_assembly_mode_trio is not None:
                        print("duplicate plasmid output: " + library_id_resistance_gene_assembly_mode_trio, file=sys.stderr)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-d', '--analysis-dir')
    parser.add_argument('-a', '--assembler', default='unicycler')
    parser.add_argument('--noheader', action='store_true')
    parser.add_argument('--output')
    parser.add_argument('--plasmid-output')
    args = parser.parse_args()
    main(args)
