#!/usr/bin/env python3

import argparse
import csv
import glob
import json
import os
import re
import sys
import yaml

from pathlib import Path

def jdump(x):
    print(json.dumps(x, indent=2))


def join_indexed_dicts(dict_1: dict, dict_2: dict) -> dict:
    """
    dict_1 is a dict of dicts, where the keys of the outer dict are the index
    dict_2 is a dict of dicts, where the keys of the outer dict are the index
    Returns a dict of dicts, where the keys of the outer dict are the index, and the values are the union of the inner dicts

    :param dict_1: dict of dicts
    :param dict_2: dict of dicts
    :return: dict of dicts
    :rtype: dict
    """
    joined_dict = {}
    if not dict_1:
        return dict_2
    if not dict_2:
        return dict_1
    dict_1_index = set(dict_1.keys())
    dict_2_keys = list(dict_2.values())[0].keys()
    
    for k in dict_1_index:
        joined_dict[k] = dict_1[k]
    for k in dict_1_index:
        if k in dict_2.keys():
            for k2 in dict_2_keys:
                joined_dict[k][k2] = dict_2[k][k2]
        else:
            for k2 in dict_2_keys:
                joined_dict[k][k2] = None
            
    return joined_dict


def parse_provenance(provenance_path: Path):
    """
    """
    with open(provenance_path, 'r') as f:
        try:
            parsed_provenance = yaml.safe_load(f)
        except yaml.YAMLError as e:
            print(e)
            exit(-1)
    for provenance_item in parsed_provenance:
        if 'timestamp_analysis_start' in provenance_item.keys():
            provenance_item['timestamp_analysis_start'] = str(provenance_item['timestamp_analysis_start'])

    return(parsed_provenance)


def parse_assembly_provenance(provenance_path: Path):
    """
    """
    parsed_provenance = parse_provenance(provenance_path)
    assembly_provenance = {}
    for provenance_item in parsed_provenance:
        if 'pipeline_name' in provenance_item:
            assembly_provenance['assembly_pipeline_name'] = provenance_item['pipeline_name']
        if 'pipeline_version' in provenance_item:
            assembly_provenance['assembly_pipeline_version'] = provenance_item['pipeline_version']

    return assembly_provenance


def parse_mlst_provenance(provenance_path: Path):
    """
    """
    parsed_provenance = parse_provenance(provenance_path)
    mlst_provenance = {}
    for provenance_item in parsed_provenance:
        if 'pipeline_name' in provenance_item:
            mlst_provenance['mlst_pipeline_name'] = provenance_item['pipeline_name']
        if 'pipeline_version' in provenance_item:
            mlst_provenance['mlst_pipeline_version'] = provenance_item['pipeline_version']

    return mlst_provenance


def parse_taxon_abundance_provenance(provenance_path: Path):
    """
    """
    parsed_provenance = parse_provenance(provenance_path)
    taxon_abundance_provenance = {}
    for provenance_item in parsed_provenance:
        if 'pipeline_name' in provenance_item:
            taxon_abundance_provenance['species_pipeline_name'] = provenance_item['pipeline_name']
        if 'pipeline_version' in provenance_item:
            taxon_abundance_provenance['species_pipeline_version'] = provenance_item['pipeline_version']

    return taxon_abundance_provenance


def parse_plasmid_screen_provenance(provenance_path):
    """
    """
    parsed_provenance = parse_provenance(provenance_path)
    plasmid_screen_provenance = {}
    plasmid_database_directory = None
    for provenance_item in parsed_provenance:
        if 'process_name' in provenance_item and provenance_item['process_name'] == 'mob_recon':
            for tool in provenance_item['tools']:
                if tool['tool_name'] == 'mob_recon':
                    for parameter in tool['parameters']:
                        if parameter['parameter'] == 'database_directory':
                            plasmid_database_directory = parameter['value']

    plasmid_screen_provenance['plasmid_database_directory'] = plasmid_database_directory

    return plasmid_screen_provenance


def parse_qc_summary(qc_summary_path: Path):
    """
    """
    qc_summary = {}
    with open(qc_summary_path, 'r') as f:
        reader = csv.DictReader(f, dialect='unix')
        for row in reader:
            library_id = row['library_id']
            qc = {'library_id': library_id}
            try:
                qc['total_bases_input_short'] = int(row['total_bases_input'])
            except ValueError as e:
                row['total_bases_input'] = None
            if 'total_bases_input_long' in row.keys():
                try:
                    qc['total_bases_input_long'] = int(row['total_bases_input_long'])
                except ValueError as e:
                    qc['total_bases_input_long'] = None
            else:
                qc['total_bases_input_long'] = 0
            if 'total_bases_input_pass_fail' in row.keys():
                qc['total_bases_input_short_pass_fail'] = row['total_bases_input_pass_fail']
            if 'total_bases_input_long_pass_fail' in row.keys():
                qc['total_bases_input_long_pass_fail'] = row['total_bases_input_long_pass_fail']
            else:
                qc['total_bases_input_long_pass_fail'] = None

            if qc['total_bases_input_long_pass_fail'] == None and 'total_bases_input_short_pass_fail' in qc:
                qc['overall_pass_fail'] = qc['total_bases_input_short_pass_fail']
            elif qc['total_bases_input_long_pass_fail'] == None:
                if qc['total_bases_input_short'] > 250_000_000:
                    qc['total_bases_input_short_pass_fail'] = 'PASS'
                else:
                    qc['total_bases_input_short_pass_fail'] = 'FAIL'
                qc['overall_pass_fail'] = qc['total_bases_input_short_pass_fail']
            else:
                qc['overall_pass_fail'] = 'FAIL'
                qc_checks = [qc[field] for field in ['total_bases_input_long_pass_fail', 'total_bases_input_short_pass_fail']]
                if not any([x == 'FAIL' for x in qc_checks]):
                    qc['overall_pass_fail'] = 'WARN'
                if all([x == 'PASS' for x in qc_checks]):
                    qc['overall_pass_fail'] = 'PASS'
            
            qc_summary[library_id] = qc

    return qc_summary


def parse_mlst_sequence_type(mlst_sequence_type_csv_path: Path):
    """
    """
    sequence_type = {}
    fields_to_collect = {
        'scheme':        'mlst_scheme',
        'sequence_type': 'mlst_sequence_type',
        'score':         'mlst_score',
    }
    with open(mlst_sequence_type_csv_path, 'r') as f:
        reader = csv.DictReader(f, dialect='unix')
        for row in reader:
            for original_field, output_field in fields_to_collect.items():
                sequence_type[output_field] = row[original_field]

    return sequence_type


def parse_mlst_alleles(mlst_json_path: Path):
    """
    """
    mlst_json = {}
    with open(mlst_json_path, 'r') as f:
        mlst_json = json.load(f)
    k = list(mlst_json.keys())[0]
    v = mlst_json[k]
    alleles = v['alleles']
    if alleles is not None:
        alleles_sorted = dict(sorted(alleles.items()))
    else:
        alleles_sorted = {}

    alleles = {'mlst_alleles': str(alleles_sorted)}

    return alleles


def collect_mlst_results(mlst_output_dir: Path):
    """
    """
    mlst_results_by_sample_id = {}

    sequence_type_by_sample_id = {}
    sequence_type_files = glob.glob(os.path.join(mlst_output_dir, '*', '*_sequence_type.csv'))
    for sequence_type_file in sequence_type_files:
        sample_id = os.path.basename(sequence_type_file).split('_')[0]
        sequence_type = parse_mlst_sequence_type(sequence_type_file)
        sequence_type_by_sample_id[sample_id] = sequence_type

    mlst_results_by_sample_id = sequence_type_by_sample_id.copy()

    mlst_alleles_by_sample_id = {}
    mlst_alleles_files = sequence_type_files = glob.glob(os.path.join(mlst_output_dir, '*', '*_mlst.json'))
    for mlst_alleles_file in mlst_alleles_files:
        sample_id = os.path.basename(mlst_alleles_file).split('_')[0]
        alleles = parse_mlst_alleles(mlst_alleles_file)
        mlst_alleles_by_sample_id[sample_id] = alleles

    for k, v in mlst_alleles_by_sample_id.items():
        mlst_results_by_sample_id[k].update(mlst_alleles_by_sample_id[k])

    mlst_provenance_files = glob.glob(os.path.join(mlst_output_dir, '*', '*_provenance.yml'))
    mlst_provenance_by_sample_id = {}
    for mlst_provenance_file in mlst_provenance_files:
        sample_id = os.path.basename(mlst_provenance_file).split('_')[0]
        parsed_mlst_provenance = parse_mlst_provenance(mlst_provenance_file)
        mlst_provenance_by_sample_id[sample_id] = parsed_mlst_provenance

    for k, v in mlst_provenance_by_sample_id.items():
        mlst_results_by_sample_id[k].update(mlst_provenance_by_sample_id[k])

    return mlst_results_by_sample_id


def parse_top_species(top_species_csv_path: Path):
    """
    """
    top_species = {}
    fields_to_collect = {
        'abundance_1_name':                  'species_1_name',
        'abundance_1_ncbi_taxonomy_id':      'species_1_taxid',
        'abundance_1_fraction_total_reads':  'species_1_percent',
        'abundance_2_name':                  'species_2_name',
        'abundance_2_ncbi_taxonomy_id':      'species_2_taxid',
        'abundance_2_fraction_total_reads':  'species_2_percent',
        'unclassified_fraction_total_reads': 'species_unclassified_percent',

    }
    null_output = {x: 'NA' for x in fields_to_collect.values()}

    try:
        with open(top_species_csv_path, 'r') as f:
            reader = csv.DictReader(f, dialect='unix')
            for row in reader:
                for original_field, output_field in fields_to_collect.items():
                    top_species[output_field] = row[original_field]
    except FileNotFoundError as e:
        top_species = null_output

    return top_species


def parse_abricate_report(abricate_report_tsv_path: Path) -> list[dict]:
    """
    Parse the Abricate report TSV file and return a list of dictionaries.

    :param abricate_report_tsv_path: Path to the Abricate report TSV file.
    :type abricate_report_tsv_path: Path
    :return: List of dictionaries containing the parsed Abricate report.
    :rtype: list[dict]
    """
    parsed_abricate_report = []
    fieldname_translations = {
        'sequence': 'contig_id',
    }
    int_fields = [
        'start',
        'end',
    ]
    float_fields = [
        'percent_coverage',
        'percent_identity',
    ]
    with open(abricate_report_tsv_path, 'r') as f:
        reader = csv.DictReader(f, delimiter='\t')
        for row in reader:
            abricate_report_record = {}
            original_keys = list(row.keys())
            cleaned_keys = [key.strip().lstrip('#').lower().replace('%', 'percent_') for key in row.keys()]

            for idx, cleaned_key in enumerate(cleaned_keys):
                if cleaned_key in fieldname_translations:
                    key = fieldname_translations[cleaned_key]
                else:
                    key = cleaned_key

                original_key = original_keys[idx]

                if key in int_fields:
                    try:
                        abricate_report_record[key] = int(row[original_key])
                    except ValueError:
                        abricate_report_record[key] = None
                elif key in float_fields:
                    try:
                        abricate_report_record[key] = float(row[original_key])
                    except ValueError:
                        abricate_report_record[key] = None
                else:
                    abricate_report_record[key] = row[original_key]

            parsed_abricate_report.append(abricate_report_record)

    for record in parsed_abricate_report:
        filename = record['file']
        if 'chromosome' in filename:
            contig_localization = 'chromosome'
        elif 'plasmid' in filename:
            contig_localization = 'plasmid'
        else:
            contig_localization = None

        record['contig_localization'] = contig_localization

    return parsed_abricate_report


def collect_taxon_abundance_results(taxon_abundance_output_dir: Path):
    """
    """
    taxon_abundance_results_by_sample_id = {}
    top_species_files = glob.glob(os.path.join(taxon_abundance_output_dir, '*', '*_S_top_5.csv'))
    top_species_by_sample_id = {}
    for top_species_file in top_species_files:
        sample_id = os.path.basename(top_species_file).split('_')[0]
        top_species = parse_top_species(top_species_file)
        top_species_by_sample_id[sample_id] = top_species

    taxon_abundance_results_by_sample_id = top_species_by_sample_id.copy()

    taxon_abundance_provenance_files = glob.glob(os.path.join(taxon_abundance_output_dir, '*', '*_provenance.yml'))
    taxon_abundance_provenance_by_sample_id = {}
    for taxon_abundance_provenance_file in taxon_abundance_provenance_files:
        sample_id = os.path.basename(taxon_abundance_provenance_file).split('_')[0]
        parsed_taxon_abundance_provenance = parse_taxon_abundance_provenance(taxon_abundance_provenance_file)
        taxon_abundance_provenance_by_sample_id[sample_id] = parsed_taxon_abundance_provenance

    for k, v in taxon_abundance_provenance_by_sample_id.items():
        taxon_abundance_results_by_sample_id[k].update(taxon_abundance_provenance_by_sample_id[k])

    return taxon_abundance_results_by_sample_id


def parse_resistance_gene_report(resistance_gene_report_tsv_path: Path):
    """
    """
    resistance_genes = []
    fields_to_collect = {
        'resistance_gene_id':                    'resistance_gene_id',
        'resistance_gene_contig_id':             'resistance_gene_contig_id',
        'resistance_gene_contig_size':           'resistance_gene_contig_size',
        'percent_resistance_gene_coverage':      'resistance_gene_percent_coverage',
        'percent_resistance_gene_identity':      'resistance_gene_percent_identity',
        'num_contigs_in_plasmid_reconstruction': 'plasmid_num_contigs',
        'plasmid_reconstruction_size':           'plasmid_size',
        'replicon_types':                        'plasmid_replicon_types',
        'mob_suite_primary_cluster_id':          'plasmid_mob_suite_primary_cluster_id',
        'mob_suite_secondary_cluster_id':        'plasmid_mob_suite_secondary_cluster_id',
        'mash_nearest_neighbor':                 'plasmid_mash_nearest_neighbor',
        'alignment_ref_plasmid':                 'plasmid_alignment_ref',
        'depth_coverage_threshold':              'plasmid_alignment_depth_threshold',
        'percent_ref_plasmid_coverage_above_depth_threshold': 'plasmid_alignment_percent_coverage_above_depth_threshold',
        'num_snps_vs_ref_plasmid':               'plasmid_alignment_num_snps',
    }
    null_output = {x: 'NA' for x in fields_to_collect.values()}

    try:
        with open(resistance_gene_report_tsv_path, 'r') as f:
            reader = csv.DictReader(f, dialect='excel-tab')
            for row in reader:
                gene = {}
                for original_field, output_field in fields_to_collect.items():
                    gene[output_field] = row[original_field]
                localization = row['assembly_file'].split('.')[0].split('_')[1]
                gene['resistance_gene_contig_localization'] = localization
                resistance_genes.append(gene)
    except FileNotFoundError as e:
        resistance_genes = [null_output]

    return resistance_genes


def collect_plasmid_results(plasmid_screen_output_dir: Path):
    """
    Collect plasmid results from a directory of plasmid screen output files

    :param plasmid_screen_output_dir: path to directory containing plasmid screen output files
    :type plasmid_screen_output_dir: Path
    :return: dict of plasmid results
    :rtype: dict
    """
    plasmid_results_by_sample_id = {}

    plasmid_screen_provenance_files = glob.glob(os.path.join(plasmid_screen_output_dir, '*', '*_provenance.yml'))
    first_plasmid_screen_provenance_file = plasmid_screen_provenance_files[0]
    plasmid_screen_provenance = parse_plasmid_screen_provenance(first_plasmid_screen_provenance_file)
    plasmid_database_directory = plasmid_screen_provenance['plasmid_database_directory']
    plasmid_database_metadata_file = os.path.join(plasmid_database_directory, 'metadata.json')
    plasmid_database_metadata = {
        'dbname': None,
        'version': None,
        'date': None,
    }
    if os.path.exists(plasmid_database_metadata_file):
        with open(plasmid_database_metadata_file, 'r') as f:
            plasmid_database_metadata = json.load(f)
    
    resistance_gene_report_files = glob.glob(os.path.join(plasmid_screen_output_dir, '*', '*_resistance_gene_report.tsv'))
    for resistance_gene_report_file in resistance_gene_report_files:
        sample_id = os.path.basename(resistance_gene_report_file).split('_')[0]
        resistance_genes = parse_resistance_gene_report(resistance_gene_report_file)
        plasmid_results_by_sample_id[sample_id] = resistance_genes
        for sample_plasmid_results in plasmid_results_by_sample_id[sample_id]:
            sample_plasmid_results['plasmid_database_name'] = plasmid_database_metadata['dbname']
            sample_plasmid_results['plasmid_database_version'] = plasmid_database_metadata['version']
            sample_plasmid_results['plasmid_database_date'] = plasmid_database_metadata['date']

    # In cases where no plasmid contigs are found, the carbapenemase may not be present in
    # the resistance gene report files. We can recover these carbapenemase gene detections
    # by taking them directly from the abricate reports.
    chromosomal_carbapenemases_by_sample_id = {}

    abricate_report_files = glob.glob(os.path.join(plasmid_screen_output_dir, '*', '*_abricate.tsv'))
    abricate_ncbi_report_files = glob.glob(os.path.join(plasmid_screen_output_dir, '*', '*_abricate_ncbi.tsv'))
    all_abricate_report_files = abricate_report_files + abricate_ncbi_report_files
    for abricate_report_file in all_abricate_report_files:
        sample_id = os.path.basename(abricate_report_file).split('_')[0]
        abricate_report = parse_abricate_report(abricate_report_file)
        abricate_carbapenemases = [x for x in abricate_report if 'CARBAPENEM' in x['resistance']]
        chromosomal_carbapenemases = [x for x in abricate_carbapenemases if 'chromosome' in x['contig_localization']]
        chromosomal_carbapenemases_by_sample_id[sample_id] = chromosomal_carbapenemases

    # First check to see if the chromosomal carbapenemase is already present
    # in the resistance gene report. This is commonly true, as long as some
    # plasmid contigs were found by mob-suite
    for sample_id, chromosomal_carbapenemases in chromosomal_carbapenemases_by_sample_id.items():
        for chromosomal_carbapenemase in chromosomal_carbapenemases:
            chromosomal_carbapenemase_gene_id = chromosomal_carbapenemase['gene']
            chromosomal_contig_id = chromosomal_carbapenemase['contig_id']
            chromosomal_carbapenemase['present_in_plasmid_results'] = False
            fields_to_compare = {
                'gene': 'resistance_gene_id',
                'contig_id': 'resistance_gene_contig_id',
            }
            if sample_id not in plasmid_results_by_sample_id:
                break
            for plasmid_result in plasmid_results_by_sample_id[sample_id]:
                match_found = all([chromosomal_carbapenemase[chromosomal_key] == plasmid_result[plasmid_result_key] for chromosomal_key, plasmid_result_key in fields_to_compare.items()])
                if match_found:
                    chromosomal_carbapenemase['present_in_plasmid_results'] = True

    # For any chromosomal carbapenemase that was not included in the resistance gene report,
    # include it here. Even though it isn't strictly a 'plasmid result', this is where
    # We report any carbapenemase gene found.
    for sample_id, chromosomal_carbapenemases in chromosomal_carbapenemases_by_sample_id.items():
        for chromosomal_carbapenemase in chromosomal_carbapenemases:
            if not chromosomal_carbapenemase['present_in_plasmid_results']:
                if sample_id not in plasmid_results_by_sample_id:
                    plasmid_results_by_sample_id[sample_id] = []
                # Unfortunately the abricate report doesn't include the total contig size so we'll
                # need to look that up in the mobtyper contig report
                resistance_gene_contig_size = '-'
                resistance_gene_contig_id = chromosomal_carbapenemase['contig_id']
                mobtyper_contig_report_path = os.path.join(plasmid_screen_output_dir, sample_id, f"{sample_id}_mobtyper_contig_report.tsv")
                if os.path.exists(mobtyper_contig_report_path):
                    with open(mobtyper_contig_report_path, 'r') as f:
                        reader = csv.DictReader(f, delimiter='\t')
                        for row in reader:
                            contig_report_contig_id = row['contig_id']
                            fixed_contig_report_contig_id = '_'.join(contig_report_contig_id.split('_')[0:2])
                            if fixed_contig_report_contig_id == resistance_gene_contig_id:
                                resistance_gene_contig_size = int(row['size'])
                plasmid_result = {
                    'resistance_gene_id': chromosomal_carbapenemase['gene'],
                    'resistance_gene_contig_id': chromosomal_carbapenemase['contig_id'],
                    'resistance_gene_contig_size': resistance_gene_contig_size,
                    'resistance_gene_contig_localization': chromosomal_carbapenemase['contig_localization'],
                    'resistance_gene_percent_coverage': chromosomal_carbapenemase['percent_coverage'],
                    'resistance_gene_percent_identity': chromosomal_carbapenemase['percent_identity'],
                    'plasmid_database_name': plasmid_database_metadata['dbname'],
                    'plasmid_database_version': plasmid_database_metadata['version'],
                    'plasmid_database_date': plasmid_database_metadata['date'],
                }
                blank_fields = [
                    'plasmid_num_contigs',
                    'plasmid_size',
                    'plasmid_replicon_types',
                    'plasmid_mob_suite_primary_cluster_id',
                    'plasmid_mob_suite_secondary_cluster_id',
                    'plasmid_mash_nearest_neighbor',
                    'plasmid_alignment_ref',
                    'plasmid_alignment_depth_threshold',
                    'plasmid_alignment_percent_coverage_above_depth_threshold',
                    'plasmid_alignment_num_snps',
                ]
                for field in blank_fields:
                    plasmid_result[field] = '-'
                plasmid_results_by_sample_id[sample_id].append(plasmid_result)


    return plasmid_results_by_sample_id


def parse_fastp_csv(fastp_csv_path):
    fastp_stats = {}
    fields_to_collect = {
        'read1_mean_length_before_filtering': 'read1_mean_length',
        'read2_mean_length_before_filtering': 'read2_mean_length',
        'q30_rate_before_filtering':          'q30_rate_short',
    }
    with open(fastp_csv_path, 'r') as f:
        reader = csv.DictReader(f, dialect='unix')
        for row in reader:
            for original_field, output_field in fields_to_collect.items():
                fastp_stats[output_field] = row[original_field]

    return fastp_stats


def parse_nanoq_csv(nanoq_csv_path):
    """
    Parse a nanoq csv file and return a dict of stats

    :param nanoq_csv_path: path to nanoq csv file
    :return: dict of stats.
    :rtype: dict
    """
    nanoq_stats = {}
    fields_to_collect = {
    }
    with open(nanoq_csv_path, 'r') as f:
        reader = csv.DictReader(f, dialect='unix')
        for row in reader:
            for original_field, output_field in fields_to_collect.items():
                nanoq_stats[output_field] = row[original_field]

    return nanoq_stats


def parse_quast_csv(quast_csv_path):
    """
    Parse a quast csv file and return a dict of stats

    :param quast_csv_path: path to quast csv file
    :return: dict of stats. Keys are: ['assembly_total_length', 'assembly_num_contigs', 'assembly_N50', 'assembly_N75']
    :rtype: dict
    """
    quast_stats = {}
    fields_to_collect = {
        'total_length': 'assembly_total_length',
        'num_contigs':  'assembly_num_contigs',
        'assembly_N50': 'assembly_N50',
        'assembly_N75': 'assembly_N75',
    }
    with open(quast_csv_path, 'r') as f:
        reader = csv.DictReader(f, dialect='unix')
        for row in reader:
            for original_field, output_field in fields_to_collect.items():
                quast_stats[output_field] = row[original_field]
    return quast_stats


def collect_assembly_results(assembly_output_dir, assembler):
    assembly_results_by_sample_id = {}

    fastp_stats_files = glob.glob(os.path.join(assembly_output_dir, '*', '*_fastp.csv'))
    
    fastp_stats_by_sample_id = {}
    for fastp_stats_file in fastp_stats_files:
        sample_id = os.path.basename(fastp_stats_file).split('_')[0]
        fastp_stats_by_sample_id[sample_id] = parse_fastp_csv(fastp_stats_file)
    
    assembly_results_by_sample_id = fastp_stats_by_sample_id.copy()

    quast_stats_by_sample_id = {}
    quast_stats_files = glob.glob(os.path.join(assembly_output_dir, '*', '*_' + assembler + '*_quast.csv'))
    for quast_stats_file in quast_stats_files:
        sample_id = os.path.basename(quast_stats_file).split('_')[0]
        quast_stats_by_sample_id[sample_id] = parse_quast_csv(quast_stats_file)

    for k, v in assembly_results_by_sample_id.items():
        if k in quast_stats_by_sample_id.keys():
            assembly_results_by_sample_id[k].update(quast_stats_by_sample_id[k])
        else:
            assembly_results_by_sample_id[k].update({
                'assembly_total_length': None,
                'assembly_num_contigs': None,
                'assembly_N50': None,
                'assembly_N75': None,
            })

    assembly_provenance_files = glob.glob(os.path.join(assembly_output_dir, '*', '*_provenance.yml'))
    
    # latest_assembly_provenance_file = assembly_provenance_files[-1]
    
    assembly_provenance_by_sample_id = {}
    for assembly_provenance_file in assembly_provenance_files:
        sample_id = os.path.basename(assembly_provenance_file).split('_')[0]
        parsed_assembly_provenance = parse_assembly_provenance(assembly_provenance_file)
        assembly_provenance_by_sample_id[sample_id] = parsed_assembly_provenance

    for k, v in assembly_results_by_sample_id.items():
        if k in assembly_provenance_by_sample_id.keys():
            assembly_results_by_sample_id[k].update(assembly_provenance_by_sample_id[k])
        else:
            assembly_results_by_sample_id[k].update({
                'assembly_pipeline_name': None,
                'assembly_pipeline_version': None,
            })
    
    return assembly_results_by_sample_id
    

def main(args):
    core_output = []
    plasmid_output = []

    miseq_run_id_regex = r'^[0-9]{6}_M[0-9]{5}_[0-9]{4}_[02]{9}-[A-Z0-9]{5}$'
    nextseq_run_id_regex = r'^[0-9]{6}_VH[0-9]{5}_[0-9]+_[A-Z0-9]{9}$'

    gridion_run_id_regex = r'^[0-9]{8}_[0-9]{4}_X[0-9]_[A-Z0-9]{8}_[a-z0-9]{8}$'
    promethion_run_id_regex = r'^[0-9]{8}_[0-9]{4}_P2S_[0-9]{5}-[0-9]_[A-Z0-9]{8}_[a-z0-9]{8}$'

    sequencing_run_id = os.path.basename(args.analysis_dir.rstrip('/'))

    instrument_type = None
    if re.match(miseq_run_id_regex, sequencing_run_id) or re.match(nextseq_run_id_regex, sequencing_run_id):
        instrument_type = 'illumina'
    elif re.match(gridion_run_id_regex, sequencing_run_id) or re.match(promethion_run_id_regex, sequencing_run_id):
        instrument_type = 'nanopore'

    assembly_output_dirname = None
    assembly_tool_name = None
    assembly_mode = None
    if instrument_type == 'illumina':
        assembly_output_dirname = 'routine-assembly-v0.4-output'
        assembly_tool_name = 'unicycler'
        assembly_mode = 'short'
    elif instrument_type == 'nanopore':
        assembly_output_dirname = 'dragonflye-nf-v0.1-output'
        assembly_tool_name = 'dragonflye'
        assembly_mode = 'hybrid'

    assembly_mode_output_dir = os.path.join(args.analysis_dir, assembly_mode)

    qc_summary = {}
    if os.path.exists(assembly_mode_output_dir):

        qc_summary_path = os.path.join(assembly_mode_output_dir, sequencing_run_id + '_auto-cpo_qc_summary.csv')

        if not os.path.exists(qc_summary_path):
            qc_summary_path = os.path.join(assembly_mode_output_dir, sequencing_run_id + '_auto-cpo_sample_qc_summary.csv')

        if os.path.exists(qc_summary_path):
            qc_summary = parse_qc_summary(qc_summary_path)
        else:
            print(f"Error: No QC summary file found for run {sequencing_run_id}")
            exit()

    core_results_by_sample_id = qc_summary.copy()

    assembly_output_subdir = os.path.join(assembly_mode_output_dir, assembly_output_dirname)
    assembly_results_by_sample_id = collect_assembly_results(assembly_output_subdir, assembly_tool_name)
    for sample_id in assembly_results_by_sample_id.keys():
        assembly_results_by_sample_id[sample_id]['assembly_mode'] = assembly_mode

    core_results_by_sample_id = join_indexed_dicts(core_results_by_sample_id, assembly_results_by_sample_id)

    mlst_output_dirname = 'mlst-nf-v0.1-output'
    mlst_output_subdir = os.path.join(assembly_mode_output_dir, mlst_output_dirname)
    mlst_results_by_sample_id = {}
    if os.path.exists(mlst_output_subdir):
        mlst_results_by_sample_id = collect_mlst_results(mlst_output_subdir)
    else:
        null_mlst_results = {
            "mlst_scheme": None,
            "mlst_sequence_type": None,
            "mlst_score": None,
            "mlst_alleles": None,
            "mlst_pipeline_name": None,
            "mlst_pipeline_version": None,
        }
        for sample_id in core_results_by_sample_id.keys():
            mlst_results_by_sample_id[sample_id] = null_mlst_results

    join_indexed_dicts(core_results_by_sample_id, mlst_results_by_sample_id)

    taxon_abundance_output_dirname = 'taxon-abundance-v0.1-output'
    taxon_abundance_output_subdir = os.path.join(assembly_mode_output_dir, taxon_abundance_output_dirname)
    taxon_abundance_results_by_sample_id = {}
    if os.path.exists(taxon_abundance_output_subdir):
        taxon_abundance_results_by_sample_id = collect_taxon_abundance_results(taxon_abundance_output_subdir)
    else:
        null_taxon_abundance_results = {
            "species_1_name": None,
            "species_1_taxid": None,
            "species_1_percent": None,
            "species_2_name": None,
            "species_2_taxid": None,
            "species_2_percent": None,
            "species_unclassified_percent": None,
            "species_pipeline_name": None,
            "species_pipeline_version": None,
        }
        for sample_id in core_results_by_sample_id.keys():
            taxon_abundance_results_by_sample_id[sample_id] = null_taxon_abundance_results

    core_results_by_sample_id = join_indexed_dicts(core_results_by_sample_id, taxon_abundance_results_by_sample_id)

    plasmid_results_by_sample_id = {}
    plasmid_screen_output_dirname = 'plasmid-screen-v0.2-output'
    plasmid_screen_output_subdir = os.path.join(assembly_mode_output_dir, plasmid_screen_output_dirname)
    plasmid_screen_results_by_sample_id = collect_plasmid_results(plasmid_screen_output_subdir)

    plasmid_results_by_sample_id = plasmid_screen_results_by_sample_id.copy()

    for k, v in core_results_by_sample_id.items():
        v['sequencing_run_id'] = sequencing_run_id
        v['library_id'] = k
        v['assembly_tool_name'] = args.assembler
        v['assembly_mode'] = assembly_mode
        v['carbapenemase_detected'] = False
        if v['library_id'] in plasmid_results_by_sample_id:
            v['carbapenemase_detected'] = True
        core_output.append(v)

    output_fieldnames = [
        'sequencing_run_id',
        'library_id',
        'total_bases_input_short',
        'total_bases_input_short_pass_fail',
        'total_bases_input_long',
        'total_bases_input_long_pass_fail',
        'overall_pass_fail',
        'carbapenemase_detected',
        'read1_mean_length',
        'read2_mean_length',
        'q30_rate_short',
        'total_reads_long',
        'total_bases_long',
        'read_mean_length_long',
        'read_n50_long',
        'median_quality_long',
        'assembly_total_length',
        'assembly_num_contigs',
        'assembly_N50',
        'assembly_N75',
        'assembly_pipeline_name',
        'assembly_pipeline_version',
        'assembly_tool_name',
        'assembly_mode',
        'mlst_scheme',
        'mlst_sequence_type',
        'mlst_score',
        'mlst_alleles',
        'mlst_pipeline_name',
        'mlst_pipeline_version',
        'species_1_name',
        'species_1_taxid',
        'species_1_percent',
        'species_2_name',
        'species_2_taxid',
        'species_2_percent',
        'species_unclassified_percent',
        'species_pipeline_name',
        'species_pipeline_version',
    ]

    # Make sure that if the overall QC check fails, other outputs are set to None
    for idx, line in enumerate(core_output):
        sequencing_run_id = line['sequencing_run_id']
        library_id = line['library_id']
        total_bases_input_short = line['total_bases_input_short']
        total_bases_input_short_pass_fail = line['total_bases_input_short_pass_fail']
        total_bases_input_long = line['total_bases_input_long']
        total_bases_input_long_pass_fail = line['total_bases_input_long_pass_fail']
        overall_pass_fail = line['overall_pass_fail']
        if overall_pass_fail == 'FAIL':
            line = {k: None for k in line.keys()}
            line['sequencing_run_id'] = sequencing_run_id
            line['library_id'] = library_id
            line['total_bases_input_short'] = total_bases_input_short
            line['total_bases_input_short_pass_fail'] = total_bases_input_short_pass_fail
            line['total_bases_input_long'] = total_bases_input_long
            line['total_bases_input_long_pass_fail'] = total_bases_input_long_pass_fail
            line['overall_pass_fail'] = overall_pass_fail
            core_output[idx] = line
            
    if not args.output:
        output_writer = csv.DictWriter(sys.stdout, fieldnames=output_fieldnames, dialect='excel-tab', quoting=csv.QUOTE_MINIMAL)
        if not args.noheader:
            output_writer.writeheader()
        for o in core_output:
            output_writer.writerow(o)
    else:
        with open(args.output, 'w') as f:
            output_writer = csv.DictWriter(f, fieldnames=output_fieldnames, dialect='excel-tab', quoting=csv.QUOTE_MINIMAL)
            if not args.noheader:
                output_writer.writeheader()
            for o in core_output:
                output_writer.writerow(o)


    for k, v in core_results_by_sample_id.items():
        if k in plasmid_results_by_sample_id.keys():
            for plasmid_result in plasmid_results_by_sample_id[k]:
                plasmid_result['total_bases_input_short'] = v['total_bases_input_short']
                plasmid_result['total_bases_input_short_pass_fail'] = v['total_bases_input_short_pass_fail']
                plasmid_result['total_bases_input_long'] = v['total_bases_input_long']
                plasmid_result['total_bases_input_long_pass_fail'] = v['total_bases_input_long_pass_fail']
                plasmid_result['overall_pass_fail'] = v['overall_pass_fail']
        else:
            plasmid_results_by_sample_id[k] = [
                {
                    'total_bases_input_short': v['total_bases_input_short'],
                    'total_bases_input_short_pass_fail': v['total_bases_input_short_pass_fail'],
                    'total_bases_input_long': v['total_bases_input_long'],
                    'total_bases_input_long_pass_fail': v['total_bases_input_long_pass_fail'],
                    'overall_pass_fail': v['overall_pass_fail'],
                }
            ]
            
    for k, vs in plasmid_results_by_sample_id.items():
        for v in vs:
            v['sequencing_run_id'] = sequencing_run_id
            v['library_id'] = k
            v['assembly_tool_name'] = args.assembler
            v['assembly_mode'] = assembly_mode
            plasmid_output.append(v)

    plasmid_output_fieldnames = [
        'sequencing_run_id',
        'library_id',
        'total_bases_input_short',
        'total_bases_input_short_pass_fail',
        'total_bases_input_long',
        'total_bases_input_long_pass_fail',
        'overall_pass_fail',
        'assembly_tool_name',
        'assembly_mode',
        'resistance_gene_id',
        'resistance_gene_contig_id',
        'resistance_gene_contig_size',
        'resistance_gene_contig_localization',
        'resistance_gene_percent_coverage',
        'resistance_gene_percent_identity',
        'plasmid_num_contigs',
        'plasmid_size',
        'plasmid_replicon_types',
        'plasmid_mob_suite_primary_cluster_id',
        'plasmid_mob_suite_secondary_cluster_id',
        'plasmid_mash_nearest_neighbor',
        'plasmid_alignment_ref',
        'plasmid_alignment_depth_threshold',
        'plasmid_alignment_percent_coverage_above_depth_threshold',
        'plasmid_alignment_num_snps',
        'plasmid_database_name',
        'plasmid_database_version',
        'plasmid_database_date',
    ]

    for idx, line in enumerate(plasmid_output):
        sequencing_run_id = line['sequencing_run_id']
        library_id = line['library_id']
        total_bases_input_short = line['total_bases_input_short']
        total_bases_input_short_pass_fail = line['total_bases_input_short_pass_fail']
        total_bases_input_long = line['total_bases_input_long']
        total_bases_input_long_pass_fail = line['total_bases_input_long_pass_fail']
        overall_pass_fail = line['overall_pass_fail']
        if overall_pass_fail == 'FAIL':
            line = {k: None for k in line.keys()}
            line['sequencing_run_id'] = sequencing_run_id
            line['library_id'] = library_id
            line['total_bases_input_short'] = total_bases_input_short
            line['total_bases_input_short_pass_fail'] = total_bases_input_short_pass_fail
            line['total_bases_input_long'] = total_bases_input_long
            line['total_bases_input_long_pass_fail'] = total_bases_input_long_pass_fail
            line['overall_pass_fail'] = overall_pass_fail
            plasmid_output[idx] = line

    written_plasmid_rows = set()
    if args.plasmid_output:
        with open(args.plasmid_output, 'w') as f:
            plasmid_output_writer = csv.DictWriter(f, fieldnames=plasmid_output_fieldnames, dialect='excel-tab', quoting=csv.QUOTE_MINIMAL)
            if not args.noheader:
                plasmid_output_writer.writeheader()
            for o in plasmid_output:
                if o['overall_pass_fail'] != 'FAIL' and 'library_id' in o and 'resistance_gene_id' in o and 'assembly_mode' in o:
                    library_id_resistance_gene_assembly_mode_trio = o['library_id'] + '-' + o['resistance_gene_id'] + '-' + o['assembly_mode']
                else:
                    library_id_resistance_gene_assembly_mode_trio = None
                if library_id_resistance_gene_assembly_mode_trio not in written_plasmid_rows:
                    plasmid_output_writer.writerow(o)
                    written_plasmid_rows.add(library_id_resistance_gene_assembly_mode_trio)
                else:
                    if library_id_resistance_gene_assembly_mode_trio is not None:
                        print("duplicate plasmid output: " + library_id_resistance_gene_assembly_mode_trio, file=sys.stderr)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-d', '--analysis-dir')
    parser.add_argument('-a', '--assembler', default='unicycler')
    parser.add_argument('--noheader', action='store_true')
    parser.add_argument('--output')
    parser.add_argument('--plasmid-output')
    args = parser.parse_args()
    main(args)
