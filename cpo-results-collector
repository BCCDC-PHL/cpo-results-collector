#!/usr/bin/env python3

import argparse
import csv
import glob
import json
import os
import re
import sys
import yaml


def jdump(x):
    print(json.dumps(x, indent=2))


def join_indexed_dicts(dict_1, dict_2):
    """
    dict_1 is a dict of dicts, where the keys of the outer dict are the index
    dict_2 is a dict of dicts, where the keys of the outer dict are the index
    Returns a dict of dicts, where the keys of the outer dict are the index, and the values are the union of the inner dicts

    :param dict_1: dict of dicts
    :param dict_2: dict of dicts
    :return: dict of dicts
    :rtype: dict
    """
    joined_dict = {}
    if not dict_1:
        return dict_2
    if not dict_2:
        return dict_1
    dict_1_index = set(dict_1.keys())
    dict_2_keys = list(dict_2.values())[0].keys()
    
    for k in dict_1_index:
        joined_dict[k] = dict_1[k]
    for k in dict_1_index:
        if k in dict_2.keys():
            for k2 in dict_2_keys:
                joined_dict[k][k2] = dict_2[k][k2]
        else:
            for k2 in dict_2_keys:
                joined_dict[k][k2] = None
            
    return joined_dict


def parse_provenance(provenance_path):
    with open(provenance_path, 'r') as f:
        try:
            parsed_provenance = yaml.safe_load(f)
        except yaml.YAMLError as e:
            print(e)
            exit(-1)
    for provenance_item in parsed_provenance:
        if 'timestamp_analysis_start' in provenance_item.keys():
            provenance_item['timestamp_analysis_start'] = str(provenance_item['timestamp_analysis_start'])

    return(parsed_provenance)


def parse_assembly_provenance(provenance_path):
    parsed_provenance = parse_provenance(provenance_path)
    assembly_provenance = {}
    for provenance_item in parsed_provenance:
        if 'pipeline_name' in provenance_item:
            assembly_provenance['assembly_pipeline_name'] = provenance_item['pipeline_name']
        if 'pipeline_version' in provenance_item:
            assembly_provenance['assembly_pipeline_version'] = provenance_item['pipeline_version']

    return assembly_provenance


def parse_mlst_provenance(provenance_path):
    parsed_provenance = parse_provenance(provenance_path)
    mlst_provenance = {}
    for provenance_item in parsed_provenance:
        if 'pipeline_name' in provenance_item:
            mlst_provenance['mlst_pipeline_name'] = provenance_item['pipeline_name']
        if 'pipeline_version' in provenance_item:
            mlst_provenance['mlst_pipeline_version'] = provenance_item['pipeline_version']

    return mlst_provenance


def parse_taxon_abundance_provenance(provenance_path):
    parsed_provenance = parse_provenance(provenance_path)
    taxon_abundance_provenance = {}
    for provenance_item in parsed_provenance:
        if 'pipeline_name' in provenance_item:
            taxon_abundance_provenance['species_pipeline_name'] = provenance_item['pipeline_name']
        if 'pipeline_version' in provenance_item:
            taxon_abundance_provenance['species_pipeline_version'] = provenance_item['pipeline_version']

    return taxon_abundance_provenance


def parse_qc_summary(qc_summary_path):
    qc_summary = {}
    with open(qc_summary_path, 'r') as f:
        reader = csv.DictReader(f, dialect='unix')
        for row in reader:
            library_id = row['library_id']
            qc = {'library_id': library_id}
            try:
                qc['total_bases_input_short'] = int(row['total_bases_input'])
            except ValueError as e:
                row['total_bases_input'] = None
            if 'total_bases_input_long' in row.keys():
                try:
                    qc['total_bases_input_long'] = int(row['total_bases_input_long'])
                except ValueError as e:
                    qc['total_bases_input_long'] = None
            else:
                qc['total_bases_input_long'] = 0
            if 'total_bases_input_pass_fail' in row.keys():
                qc['total_bases_input_short_pass_fail'] = row['total_bases_input_pass_fail']
            if 'total_bases_input_long_pass_fail' in row.keys():
                qc['total_bases_input_long_pass_fail'] = row['total_bases_input_long_pass_fail']
            else:
                qc['total_bases_input_long_pass_fail'] = None

            if qc['total_bases_input_long_pass_fail'] == None:
                qc['overall_pass_fail'] = qc['total_bases_input_short_pass_fail']
            else:
                qc['overall_pass_fail'] = 'FAIL'
                qc_checks = [qc[field] for field in ['total_bases_input_long_pass_fail', 'total_bases_input_short_pass_fail']]
                if not any([x == 'FAIL' for x in qc_checks]):
                    qc['overall_pass_fail'] = 'WARN'
                if all([x == 'PASS' for x in qc_checks]):
                    qc['overall_pass_fail'] = 'PASS'
            
            qc_summary[library_id] = qc

    return qc_summary


def parse_mlst_sequence_type(mlst_sequence_type_csv_path):
    sequence_type = {}
    fields_to_collect = {
        'scheme':        'mlst_scheme',
        'sequence_type': 'mlst_sequence_type',
        'score':         'mlst_score',
    }
    with open(mlst_sequence_type_csv_path, 'r') as f:
        reader = csv.DictReader(f, dialect='unix')
        for row in reader:
            for original_field, output_field in fields_to_collect.items():
                sequence_type[output_field] = row[original_field]

    return sequence_type


def parse_mlst_alleles(mlst_json_path):
    mlst_json = {}
    with open(mlst_json_path, 'r') as f:
        mlst_json = json.load(f)
    k = list(mlst_json.keys())[0]
    v = mlst_json[k]
    alleles = v['alleles']
    if alleles is not None:
        alleles_sorted = dict(sorted(alleles.items()))
    else:
        alleles_sorted = {}

    alleles = {'mlst_alleles': str(alleles_sorted)}

    return alleles


def collect_mlst_results(mlst_output_dir):
    mlst_results_by_sample_id = {}

    sequence_type_by_sample_id = {}
    sequence_type_files = glob.glob(os.path.join(mlst_output_dir, '*', '*_sequence_type.csv'))
    for sequence_type_file in sequence_type_files:
        sample_id = os.path.basename(sequence_type_file).split('_')[0]
        sequence_type = parse_mlst_sequence_type(sequence_type_file)
        sequence_type_by_sample_id[sample_id] = sequence_type

    mlst_results_by_sample_id = sequence_type_by_sample_id.copy()

    mlst_alleles_by_sample_id = {}
    mlst_alleles_files = sequence_type_files = glob.glob(os.path.join(mlst_output_dir, '*', '*_mlst.json'))
    for mlst_alleles_file in mlst_alleles_files:
        sample_id = os.path.basename(mlst_alleles_file).split('_')[0]
        alleles = parse_mlst_alleles(mlst_alleles_file)
        mlst_alleles_by_sample_id[sample_id] = alleles

    for k, v in mlst_alleles_by_sample_id.items():
        mlst_results_by_sample_id[k].update(mlst_alleles_by_sample_id[k])

    mlst_provenance_files = glob.glob(os.path.join(mlst_output_dir, '*', '*_provenance.yml'))
    mlst_provenance_by_sample_id = {}
    for mlst_provenance_file in mlst_provenance_files:
        sample_id = os.path.basename(mlst_provenance_file).split('_')[0]
        parsed_mlst_provenance = parse_mlst_provenance(mlst_provenance_file)
        mlst_provenance_by_sample_id[sample_id] = parsed_mlst_provenance

    for k, v in mlst_provenance_by_sample_id.items():
        mlst_results_by_sample_id[k].update(mlst_provenance_by_sample_id[k])

    return mlst_results_by_sample_id


def parse_top_species(top_species_csv_path):
    top_species = {}
    fields_to_collect = {
        'abundance_1_name':                  'species_1_name',
        'abundance_1_ncbi_taxonomy_id':      'species_1_taxid',
        'abundance_1_fraction_total_reads':  'species_1_percent',
        'abundance_2_name':                  'species_2_name',
        'abundance_2_ncbi_taxonomy_id':      'species_2_taxid',
        'abundance_2_fraction_total_reads':  'species_2_percent',
        'unclassified_fraction_total_reads': 'species_unclassified_percent',

    }
    null_output = {x: 'NA' for x in fields_to_collect.values()}

    try:
        with open(top_species_csv_path, 'r') as f:
            reader = csv.DictReader(f, dialect='unix')
            for row in reader:
                for original_field, output_field in fields_to_collect.items():
                    top_species[output_field] = row[original_field]
    except FileNotFoundError as e:
        top_species = null_output

    return top_species


def collect_taxon_abundance_results(taxon_abundance_output_dir):
    taxon_abundance_results_by_sample_id = {}
    top_species_files = glob.glob(os.path.join(taxon_abundance_output_dir, '*', '*_S_top_5.csv'))
    top_species_by_sample_id = {}
    for top_species_file in top_species_files:
        sample_id = os.path.basename(top_species_file).split('_')[0]
        top_species = parse_top_species(top_species_file)
        top_species_by_sample_id[sample_id] = top_species

    taxon_abundance_results_by_sample_id = top_species_by_sample_id.copy()

    taxon_abundance_provenance_files = glob.glob(os.path.join(taxon_abundance_output_dir, '*', '*_provenance.yml'))
    taxon_abundance_provenance_by_sample_id = {}
    for taxon_abundance_provenance_file in taxon_abundance_provenance_files:
        sample_id = os.path.basename(taxon_abundance_provenance_file).split('_')[0]
        parsed_taxon_abundance_provenance = parse_taxon_abundance_provenance(taxon_abundance_provenance_file)
        taxon_abundance_provenance_by_sample_id[sample_id] = parsed_taxon_abundance_provenance

    for k, v in taxon_abundance_provenance_by_sample_id.items():
        taxon_abundance_results_by_sample_id[k].update(taxon_abundance_provenance_by_sample_id[k])

    return taxon_abundance_results_by_sample_id


def parse_resistance_gene_report(resistance_gene_report_tsv_path):
    resistance_genes = []
    fields_to_collect = {
        'resistance_gene_id':                    'resistance_gene_id',
        'resistance_gene_contig_id':             'resistance_gene_contig_id',
        'resistance_gene_contig_size':           'resistance_gene_contig_size',
        'percent_resistance_gene_coverage':      'resistance_gene_percent_coverage',
        'percent_resistance_gene_identity':      'resistance_gene_percent_identity',
        'num_contigs_in_plasmid_reconstruction': 'plasmid_num_contigs',
        'plasmid_reconstruction_size':           'plasmid_size',
        'replicon_types':                        'plasmid_replicon_types',
        'mob_suite_primary_cluster_id':          'plasmid_mob_suite_primary_cluster_id',
        'mob_suite_secondary_cluster_id':        'plasmid_mob_suite_secondary_cluster_id',
        'mash_nearest_neighbor':                 'plasmid_mash_nearest_neighbor',
        'alignment_ref_plasmid':                 'plasmid_alignment_ref',
        'depth_coverage_threshold':              'plasmid_alignment_depth_threshold',
        'percent_ref_plasmid_coverage_above_depth_threshold': 'plasmid_alignment_percent_coverage_above_depth_threshold',
        'num_snps_vs_ref_plasmid':               'plasmid_alignment_num_snps',
    }
    null_output = {x: 'NA' for x in fields_to_collect.values()}

    try:
        with open(resistance_gene_report_tsv_path, 'r') as f:
            reader = csv.DictReader(f, dialect='excel-tab')
            for row in reader:
                gene = {}
                for original_field, output_field in fields_to_collect.items():
                    gene[output_field] = row[original_field]
                resistance_genes.append(gene)
    except FileNotFoundError as e:
        resistance_genes = [null_output]

    return resistance_genes


def collect_plasmid_results(plasmid_screen_output_dir):
    plasmid_results_by_sample_id = {}
    resistance_gene_report_files = glob.glob(os.path.join(plasmid_screen_output_dir, '*', '*_resistance_gene_report.tsv'))
    for resistance_gene_report_file in resistance_gene_report_files:
        sample_id = os.path.basename(resistance_gene_report_file).split('_')[0]
        resistance_genes = parse_resistance_gene_report(resistance_gene_report_file)
        plasmid_results_by_sample_id[sample_id] = resistance_genes

    return plasmid_results_by_sample_id


def parse_fastp_csv(fastp_csv_path):
    fastp_stats = {}
    fields_to_collect = {
        'read1_mean_length_before_filtering': 'read1_mean_length',
        'read2_mean_length_before_filtering': 'read2_mean_length',
        'q30_rate_before_filtering':          'q30_rate_short',
    }
    with open(fastp_csv_path, 'r') as f:
        reader = csv.DictReader(f, dialect='unix')
        for row in reader:
            for original_field, output_field in fields_to_collect.items():
                fastp_stats[output_field] = row[original_field]

    return fastp_stats


def parse_nanoq_csv(nanoq_csv_path):
    """
    Parse a nanoq csv file and return a dict of stats

    :param nanoq_csv_path: path to nanoq csv file
    :return: dict of stats.
    :rtype: dict
    """
    nanoq_stats = {}
    fields_to_collect = {
    }
    with open(nanoq_csv_path, 'r') as f:
        reader = csv.DictReader(f, dialect='unix')
        for row in reader:
            for original_field, output_field in fields_to_collect.items():
                nanoq_stats[output_field] = row[original_field]

    return nanoq_stats


def parse_quast_csv(quast_csv_path):
    """
    Parse a quast csv file and return a dict of stats

    :param quast_csv_path: path to quast csv file
    :return: dict of stats. Keys are: ['assembly_total_length', 'assembly_num_contigs', 'assembly_N50', 'assembly_N75']
    :rtype: dict
    """
    quast_stats = {}
    fields_to_collect = {
        'total_length': 'assembly_total_length',
        'num_contigs':  'assembly_num_contigs',
        'assembly_N50': 'assembly_N50',
        'assembly_N75': 'assembly_N75',
    }
    with open(quast_csv_path, 'r') as f:
        reader = csv.DictReader(f, dialect='unix')
        for row in reader:
            for original_field, output_field in fields_to_collect.items():
                quast_stats[output_field] = row[original_field]
    return quast_stats


def collect_assembly_results(assembly_output_dir, assembler):
    assembly_results_by_sample_id = {}

    fastp_stats_files = glob.glob(os.path.join(assembly_output_dir, '*', '*_fastp.csv'))
    
    fastp_stats_by_sample_id = {}
    for fastp_stats_file in fastp_stats_files:
        sample_id = os.path.basename(fastp_stats_file).split('_')[0]
        fastp_stats_by_sample_id[sample_id] = parse_fastp_csv(fastp_stats_file)
    
    assembly_results_by_sample_id = fastp_stats_by_sample_id.copy()

    quast_stats_by_sample_id = {}
    quast_stats_files = glob.glob(os.path.join(assembly_output_dir, '*', '*_' + assembler + '*_quast.csv'))
    for quast_stats_file in quast_stats_files:
        sample_id = os.path.basename(quast_stats_file).split('_')[0]
        quast_stats_by_sample_id[sample_id] = parse_quast_csv(quast_stats_file)

    for k, v in assembly_results_by_sample_id.items():
        if k in quast_stats_by_sample_id.keys():
            assembly_results_by_sample_id[k].update(quast_stats_by_sample_id[k])
        else:
            assembly_results_by_sample_id[k].update({
                'assembly_total_length': None,
                'assembly_num_contigs': None,
                'assembly_N50': None,
                'assembly_N75': None,
            })

    assembly_provenance_files = glob.glob(os.path.join(assembly_output_dir, '*', '*_provenance.yml'))
    
    # latest_assembly_provenance_file = assembly_provenance_files[-1]
    
    assembly_provenance_by_sample_id = {}
    for assembly_provenance_file in assembly_provenance_files:
        sample_id = os.path.basename(assembly_provenance_file).split('_')[0]
        parsed_assembly_provenance = parse_assembly_provenance(assembly_provenance_file)
        assembly_provenance_by_sample_id[sample_id] = parsed_assembly_provenance

    for k, v in assembly_results_by_sample_id.items():
        if k in assembly_provenance_by_sample_id.keys():
            assembly_results_by_sample_id[k].update(assembly_provenance_by_sample_id[k])
        else:
            assembly_results_by_sample_id[k].update({
                'assembly_pipeline_name': None,
                'assembly_pipeline_version': None,
            })
    
    return assembly_results_by_sample_id
    

def main(args):
    core_output = []
    plasmid_output = []

    miseq_run_id_regex = r'^[0-9]{6}_M[0-9]{5}_[0-9]{4}_[02]{9}-[A-Z0-9]{5}$'
    nextseq_run_id_regex = r'^[0-9]{6}_VH[0-9]{5}_[0-9]+_[A-Z0-9]{9}$'

    gridion_run_id_regex = r'^[0-9]{8}_[0-9]{4}_X[0-9]_[A-Z0-9]{8}_[a-z0-9]{8}$'
    promethion_run_id_regex = r'^[0-9]{8}_[0-9]{4}_P2S_[0-9]{5}-[0-9]_[A-Z0-9]{8}_[a-z0-9]{8}$'

    sequencing_run_id = os.path.basename(args.analysis_dir.rstrip('/'))

    instrument_type = None
    if re.match(miseq_run_id_regex, sequencing_run_id) or re.match(nextseq_run_id_regex, sequencing_run_id):
        instrument_type = 'illumina'
    elif re.match(gridion_run_id_regex, sequencing_run_id) or re.match(promethion_run_id_regex, sequencing_run_id):
        instrument_type = 'nanopore'

    assembly_output_dirname = None
    assembly_tool_name = None
    assembly_mode = None
    if instrument_type == 'illumina':
        assembly_output_dirname = 'routine-assembly-v0.4-output'
        assembly_tool_name = 'unicycler'
        assembly_mode = 'short'
    elif instrument_type == 'nanopore':
        assembly_output_dirname = 'dragonflye-nf-v0.1-output'
        assembly_tool_name = 'dragonflye'
        assembly_mode = 'hybrid'

    assembly_mode_output_dir = os.path.join(args.analysis_dir, assembly_mode)

    qc_summary = {}
    if os.path.exists(assembly_mode_output_dir):

        qc_summary_path = os.path.join(assembly_mode_output_dir, sequencing_run_id + '_auto-cpo_qc_summary.csv')
        
        qc_summary_path = os.path.join(assembly_mode_output_dir, sequencing_run_id + '_auto-cpo_qc_summary.csv')

        if os.path.exists(qc_summary_path):
            qc_summary = parse_qc_summary(qc_summary_path)

    core_results_by_sample_id = qc_summary.copy()

    assembly_output_subdir = os.path.join(assembly_mode_output_dir, assembly_output_dirname)
    assembly_results_by_sample_id = collect_assembly_results(assembly_output_subdir, assembly_tool_name)
    for sample_id in assembly_results_by_sample_id.keys():
        assembly_results_by_sample_id[sample_id]['assembly_mode'] = assembly_mode

    core_results_by_sample_id = join_indexed_dicts(core_results_by_sample_id, assembly_results_by_sample_id)

    mlst_output_dirname = 'mlst-nf-v0.1-output'
    mlst_output_subdir = os.path.join(assembly_mode_output_dir, mlst_output_dirname)
    mlst_results_by_sample_id = {}
    if os.path.exists(mlst_output_subdir):
        mlst_results_by_sample_id = collect_mlst_results(mlst_output_subdir)
    else:
        null_mlst_results = {
            "mlst_scheme": None,
            "mlst_sequence_type": None,
            "mlst_score": None,
            "mlst_alleles": None,
            "mlst_pipeline_name": None,
            "mlst_pipeline_version": None,
        }
        for sample_id in core_results_by_sample_id.keys():
            mlst_results_by_sample_id[sample_id] = null_mlst_results

    join_indexed_dicts(core_results_by_sample_id, mlst_results_by_sample_id)

    taxon_abundance_output_dirname = 'taxon-abundance-v0.1-output'
    taxon_abundance_output_subdir = os.path.join(assembly_mode_output_dir, taxon_abundance_output_dirname)
    taxon_abundance_results_by_sample_id = {}
    if os.path.exists(taxon_abundance_output_subdir):
        taxon_abundance_results_by_sample_id = collect_taxon_abundance_results(taxon_abundance_output_subdir)
    else:
        null_taxon_abundance_results = {
            "species_1_name": None,
            "species_1_taxid": None,
            "species_1_percent": None,
            "species_2_name": None,
            "species_2_taxid": None,
            "species_2_percent": None,
            "species_unclassified_percent": None,
            "species_pipeline_name": None,
            "species_pipeline_version": None,
        }
        for sample_id in core_results_by_sample_id.keys():
            taxon_abundance_results_by_sample_id[sample_id] = null_taxon_abundance_results

    core_results_by_sample_id = join_indexed_dicts(core_results_by_sample_id, taxon_abundance_results_by_sample_id)

    plasmid_results_by_sample_id = {}
    plasmid_screen_output_dirname = 'plasmid-screen-v0.2-output'
    plasmid_screen_output_subdir = os.path.join(assembly_mode_output_dir, plasmid_screen_output_dirname)
    plasmid_screen_results_by_sample_id = collect_plasmid_results(plasmid_screen_output_subdir)

    plasmid_results_by_sample_id = plasmid_screen_results_by_sample_id.copy()

    for k, v in core_results_by_sample_id.items():
        v['sequencing_run_id'] = sequencing_run_id
        v['library_id'] = k
        v['assembly_tool_name'] = args.assembler
        v['assembly_mode'] = assembly_mode
        core_output.append(v)

    output_fieldnames = [
        'sequencing_run_id',
        'library_id',
        'total_bases_input_short',
        'total_bases_input_short_pass_fail',
        'total_bases_input_long',
        'total_bases_input_long_pass_fail',
        'overall_pass_fail',
        'read1_mean_length',
        'read2_mean_length',
        'q30_rate_short',
        'total_reads_long',
        'total_bases_long',
        'read_mean_length_long',
        'read_n50_long',
        'median_quality_long',
        'assembly_total_length',
        'assembly_num_contigs',
        'assembly_N50',
        'assembly_N75',
        'assembly_pipeline_name',
        'assembly_pipeline_version',
        'assembly_tool_name',
        'assembly_mode',
        'mlst_scheme',
        'mlst_sequence_type',
        'mlst_score',
        'mlst_alleles',
        'mlst_pipeline_name',
        'mlst_pipeline_version',
        'species_1_name',
        'species_1_taxid',
        'species_1_percent',
        'species_2_name',
        'species_2_taxid',
        'species_2_percent',
        'species_unclassified_percent',
        'species_pipeline_name',
        'species_pipeline_version',
    ]

    # Make sure that if the overall QC check fails, other outputs are set to None
    for idx, line in enumerate(core_output):
        sequencing_run_id = line['sequencing_run_id']
        library_id = line['library_id']
        total_bases_input_short = line['total_bases_input_short']
        total_bases_input_short_pass_fail = line['total_bases_input_short_pass_fail']
        total_bases_input_long = line['total_bases_input_long']
        total_bases_input_long_pass_fail = line['total_bases_input_long_pass_fail']
        overall_pass_fail = line['overall_pass_fail']
        if overall_pass_fail == 'FAIL':
            line = {k: None for k in line.keys()}
            line['sequencing_run_id'] = sequencing_run_id
            line['library_id'] = library_id
            line['total_bases_input_short'] = total_bases_input_short
            line['total_bases_input_short_pass_fail'] = total_bases_input_short_pass_fail
            line['total_bases_input_long'] = total_bases_input_long
            line['total_bases_input_long_pass_fail'] = total_bases_input_long_pass_fail
            line['overall_pass_fail'] = overall_pass_fail
            core_output[idx] = line
            
    if not args.output:
        output_writer = csv.DictWriter(sys.stdout, fieldnames=output_fieldnames, dialect='excel-tab', quoting=csv.QUOTE_MINIMAL)
        if not args.noheader:
            output_writer.writeheader()
        for o in core_output:
            output_writer.writerow(o)
    else:
        with open(args.output, 'w') as f:
            output_writer = csv.DictWriter(f, fieldnames=output_fieldnames, dialect='excel-tab', quoting=csv.QUOTE_MINIMAL)
            if not args.noheader:
                output_writer.writeheader()
            for o in core_output:
                output_writer.writerow(o)


    for k, v in core_results_by_sample_id.items():
        if k in plasmid_results_by_sample_id.keys():
            for plasmid_result in plasmid_results_by_sample_id[k]:
                plasmid_result['total_bases_input_short'] = v['total_bases_input_short']
                plasmid_result['total_bases_input_short_pass_fail'] = v['total_bases_input_short_pass_fail']
                plasmid_result['total_bases_input_long'] = v['total_bases_input_long']
                plasmid_result['total_bases_input_long_pass_fail'] = v['total_bases_input_long_pass_fail']
                plasmid_result['overall_pass_fail'] = v['overall_pass_fail']
        else:
            plasmid_results_by_sample_id[k] = [
                {
                    'total_bases_input_short': v['total_bases_input_short'],
                    'total_bases_input_short_pass_fail': v['total_bases_input_short_pass_fail'],
                    'total_bases_input_long': v['total_bases_input_long'],
                    'total_bases_input_long_pass_fail': v['total_bases_input_long_pass_fail'],
                    'overall_pass_fail': v['overall_pass_fail'],
                }
            ]
            
    for k, vs in plasmid_results_by_sample_id.items():
        for v in vs:
            v['sequencing_run_id'] = sequencing_run_id
            v['library_id'] = k
            v['assembly_tool_name'] = args.assembler
            v['assembly_mode'] = assembly_mode
            plasmid_output.append(v)

    plasmid_output_fieldnames = [
        'sequencing_run_id',
        'library_id',
        'total_bases_input_short',
        'total_bases_input_short_pass_fail',
        'total_bases_input_long',
        'total_bases_input_long_pass_fail',
        'overall_pass_fail',
        'assembly_tool_name',
        'assembly_mode',
        'resistance_gene_id',
        'resistance_gene_contig_id',
        'resistance_gene_contig_size',
        'resistance_gene_percent_coverage',
        'resistance_gene_percent_identity',
        'plasmid_num_contigs',
        'plasmid_size',
        'plasmid_replicon_types',
        'plasmid_mob_suite_primary_cluster_id',
        'plasmid_mob_suite_secondary_cluster_id',
        'plasmid_mash_nearest_neighbor',
        'plasmid_alignment_ref',
        'plasmid_alignment_depth_threshold',
        'plasmid_alignment_percent_coverage_above_depth_threshold',
        'plasmid_alignment_num_snps',
    ]

    for idx, line in enumerate(plasmid_output):
        sequencing_run_id = line['sequencing_run_id']
        library_id = line['library_id']
        total_bases_input_short = line['total_bases_input_short']
        total_bases_input_short_pass_fail = line['total_bases_input_short_pass_fail']
        total_bases_input_long = line['total_bases_input_long']
        total_bases_input_long_pass_fail = line['total_bases_input_long_pass_fail']
        overall_pass_fail = line['overall_pass_fail']
        if overall_pass_fail == 'FAIL':
            line = {k: None for k in line.keys()}
            line['sequencing_run_id'] = sequencing_run_id
            line['library_id'] = library_id
            line['total_bases_input_short'] = total_bases_input_short
            line['total_bases_input_short_pass_fail'] = total_bases_input_short_pass_fail
            line['total_bases_input_long'] = total_bases_input_long
            line['total_bases_input_long_pass_fail'] = total_bases_input_long_pass_fail
            line['overall_pass_fail'] = overall_pass_fail
            plasmid_output[idx] = line

    written_plasmid_rows = set()
    if args.plasmid_output:
        with open(args.plasmid_output, 'w') as f:
            plasmid_output_writer = csv.DictWriter(f, fieldnames=plasmid_output_fieldnames, dialect='excel-tab', quoting=csv.QUOTE_MINIMAL)
            if not args.noheader:
                plasmid_output_writer.writeheader()
            for o in plasmid_output:
                if o['overall_pass_fail'] != 'FAIL' and 'library_id' in o and 'resistance_gene_id' in o and 'assembly_mode' in o:
                    library_id_resistance_gene_assembly_mode_trio = o['library_id'] + '-' + o['resistance_gene_id'] + '-' + o['assembly_mode']
                else:
                    library_id_resistance_gene_assembly_mode_trio = None
                if library_id_resistance_gene_assembly_mode_trio not in written_plasmid_rows:
                    plasmid_output_writer.writerow(o)
                    written_plasmid_rows.add(library_id_resistance_gene_assembly_mode_trio)
                else:
                    if library_id_resistance_gene_assembly_mode_trio is not None:
                        print("duplicate plasmid output: " + library_id_resistance_gene_assembly_mode_trio, file=sys.stderr)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-d', '--analysis-dir')
    parser.add_argument('-a', '--assembler', default='unicycler')
    parser.add_argument('--noheader', action='store_true')
    parser.add_argument('--output')
    parser.add_argument('--plasmid-output')
    args = parser.parse_args()
    main(args)
